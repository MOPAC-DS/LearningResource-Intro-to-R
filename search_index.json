[["index.html", "Introduction to R for Crime Analysts Preface Purpose of this Book About the Author", " Introduction to R for Crime Analysts Daniel Hammocks, Senior Data Scientist at Mayor’s Office for Policing and Crime 2024-08-28 Preface “Programming doesn’t just solve problems; it opens doors to new possibilities, unlocking the potential to create, innovate, and transform the world around us.” - Daniel Hammocks (2024) All good books seem to start with an inspirational quote… I stole that one from ChatGPT – with its permission! Purpose of this Book In the world of data analysis, the choice of software can significantly impact the flexibility, efficiency, and depth of your analyses. For many years, SPSS has been a popular tool among social scientists, market researchers, and others who require a straightforward interface for statistical analysis. However, as data analysis becomes more complex and expansive, many analysts are turning to R, an open-source programming language and environment that offers unparalleled flexibility, a vast array of packages, and a growing community of users. This course is designed to help you transition from SPSS to R, demonstrating that all of the functionality you’re accustomed to in SPSS can be replicated—and often enhanced—in R. By the end of this course, you will be equipped with the knowledge and skills to conduct your analyses in R, whether you’re dealing with basic descriptive statistics, survey data, or more advanced statistical models. About the Author Daniel Hammocks is a PhD Candidate in Applied Data Science, specialising in the use of natural language processing to identify emerging methods of criminal perpetration from unstructured free-text crime reports. With over 10 years of programming and Data Science experience, Daniel has been actively researching in the field of Security and Crime Science for more than 6 years. His expertise extends to teaching data science at both an undergraduate and master’s level, where he is deeply committed to upskilling the next generation of crime analysts by empowering them to harness the full potential of data-driven techniques. Currently, Daniel serves as a Senior Data Scientist at the Mayor’s Office for Policing and Crime (MOPAC), where he applies his extensive skills to enhance public safety and security through data-driven insights. - Also written by ChatGPT (2024)… it can have credit for this one. "],["introduction.html", "1 Introduction 1.1 Overview 1.2 Why Learn R? 1.3 Replicating SPSS Functionality in R 1.4 Transitioning from SPSS to R 1.5 Conclusion", " 1 Introduction 1.1 Overview In the world of data analysis, the choice of software can significantly impact the flexibility, efficiency, and depth of your analyses. For many years, SPSS has been a popular tool among social scientists, market researchers, and others who require a straightforward interface for statistical analysis. However, as data analysis becomes more complex and expansive, many analysts are turning to R, an open-source programming language and environment that offers unparalleled flexibility, a vast array of packages, and a growing community of users. This course is designed to help you transition from SPSS to R, demonstrating that all of the functionality you’re accustomed to in SPSS can be replicated—and often enhanced—in R. By the end of this course, you will be equipped with the knowledge and skills to conduct your analyses in R, whether you’re dealing with basic descriptive statistics, survey data, or more advanced statistical models. 1.2 Why Learn R? 1.2.1 Flexibility and Power R is a full-fledged programming language, which means it is inherently more flexible than SPSS. While SPSS provides a user-friendly interface with point-and-click options, R allows for a deeper level of customisation and control over your analyses. You can automate tasks, create reproducible reports, and even develop new statistical methods if needed. Example: In SPSS, you might run a regression using a dialog box and then manually record the output. In R, you can automate this process, run multiple regressions in a loop, and automatically output the results to a report. 1.2.2 Reproducibility Reproducibility is a cornerstone of modern data analysis. R’s script-based workflow ensures that every step of your analysis is documented, making it easy to reproduce results or adjust your analysis if new data becomes available. This contrasts with SPSS, where much of the analysis is done through a GUI, making it harder to track and reproduce each step without explicitly saving syntax files. Example: In R, you can save your entire analysis pipeline in a script, which can be rerun with new data or shared with colleagues for replication. 1.2.3 Extensive Community and Package Ecosystem One of R’s greatest strengths is its vast and active community, which has contributed thousands of packages to CRAN (the Comprehensive R Archive Network). Whether you need advanced statistical techniques, machine learning algorithms, or specialised visualisations, there’s likely an R package available. This is in contrast to SPSS, where functionality is often limited to what is provided out-of-the-box or through costly add-ons. Example: In R, you might use the ggplot2 package for advanced visualisations, dplyr for data manipulation, or survey for complex survey analysis. In SPSS, creating custom visualisations or handling complex survey designs might require more manual effort or external software. 1.2.4 Cost R is completely free and open-source, which is a significant advantage over SPSS, especially for organisations with budget constraints. This means you can install R on as many computers as you need, share it with colleagues, and access the latest updates and packages without any cost. Example: Many organisations are moving to R not only because of its capabilities but also because it reduces software costs significantly. 1.3 Replicating SPSS Functionality in R If you’re used to SPSS, the idea of switching to a command-line-driven environment might seem daunting. However, you’ll find that every major feature of SPSS can be replicated in R—often with greater flexibility and power. 1.3.1 Data Management In SPSS, data management tasks such as merging datasets, recoding variables, or selecting cases are performed using a series of dialog boxes or syntax commands. In R, these tasks are handled with functions and packages like dplyr, which offer intuitive syntax for manipulating data. Example: Recoding variables in SPSS might require several steps in the GUI, while in R, you can accomplish the same with a single line of code using mutate() and case_when(). 1.3.2 Descriptive Statistics Both SPSS and R allow you to calculate descriptive statistics such as means, medians, and standard deviations. In R, you can use basic functions like mean() and sd(), or you can employ packages like psych or skimr for more detailed summaries. Example: Descriptive statistics in SPSS are often displayed in tables generated by the DESCRIPTIVES command, whereas in R, you can achieve this with a simple script, and even automate it for multiple variables. 1.3.3 Statistical Tests SPSS is known for its user-friendly interfaces for running statistical tests. In R, all the same tests (t-tests, ANOVA, chi-square, etc.) are available, and you can conduct them using straightforward commands. Example: Running a t-test in SPSS involves navigating through multiple menus, while in R, the same test can be executed with a simple t.test() function. 1.3.4 Regression Analysis Regression analysis, a staple of SPSS, is fully supported in R. Whether you’re running simple linear regressions or more complex logistic regressions, R provides the tools you need. The lm() function is used for linear models, while glm() handles generalised linear models, including logistic regression. Example: In SPSS, you might need to manually specify each option in the regression dialog box. In R, you have the flexibility to customise your models directly in code, making it easy to adjust your analysis as needed. 1.3.5 Data Visualisation While SPSS provides basic charting capabilities, R, with its ggplot2 package, is unmatched in the realm of data visualisation. You can create everything from simple bar charts to complex multi-faceted plots, with full control over every aspect of the appearance. Example: A simple bar chart in SPSS might look quite basic, whereas in R, you can use ggplot2 to add layers, colors, themes, and annotations to create publication-quality graphics. 1.4 Transitioning from SPSS to R Transitioning from SPSS to R might feel like a big leap, but with the right guidance, you’ll soon see how R’s power and flexibility make it a worthwhile switch. Throughout this book, we will replicate common SPSS procedures in R, step by step. You’ll see how to import your data, perform the same analyses you’re used to, and even go beyond what SPSS can offer. 1.4.1 Building Confidence in R We’ll start with simple tasks, such as descriptive statistics and data manipulation, gradually moving to more advanced topics like regression analysis and survey data handling. By building on your existing knowledge of SPSS, you’ll find that learning R is not as daunting as it might seem. Example: We’ll compare SPSS and R workflows for common tasks, showing how R can simplify and enhance your analytical processes. 1.4.2 Leveraging R’s Ecosystem One of the goals of this course is to familiarise you with the rich ecosystem of R packages. We’ll introduce you to some of the most useful packages for data analysis, showing how they can replace or enhance the tools you’re used to in SPSS. Example: For instance, you’ll see how dplyr can streamline your data manipulation tasks, or how ggplot2 can elevate your data visualisation. 1.5 Conclusion As you proceed through this course, you’ll gain the skills needed to replicate and enhance the analyses you currently perform in SPSS. Each chapter will build on the previous ones, gradually expanding your knowledge and capabilities in R. By the end of this book, you’ll be able to handle all your data analysis tasks in R with confidence, taking full advantage of its flexibility, power, and vast ecosystem of packages. "],["getting-started-with-r.html", "2 Getting Started with R 2.1 The R Environment 2.2 Introduction to R Packages and Installing Key Packages 2.3 Coding Conventions and Best Practices 2.4 Data Types and Structures 2.5 Basic Operations and Functions in R 2.6 Conclusion", " 2 Getting Started with R 2.1 The R Environment 2.1.1 Overview of the RStudio Interface RStudio is a powerful integrated development environment (IDE) that provides a user-friendly interface for working with R. Understanding its layout will help you navigate and utilise its features effectively. Script Editor: This is where you write and save your R scripts. You can execute lines of code or entire scripts from here. Console: The Console displays the output of your commands and allows you to run R code directly. It’s useful for quick calculations and immediate feedback. Environment/History Pane: The Environment pane shows the objects (e.g., data frames, variables) in your current R session. The History tab logs commands you’ve executed. Files/Plots/Packages/Help Pane: This multi-functional pane lets you manage files, view plots, install/load packages, and access R’s help system. Figure 2.1: RStudio IDE Layout 2.1.2 Console vs. Scripts vs. Notebooks Console: Ideal for quick calculations and testing small pieces of code. Commands typed here are executed immediately. Scripts: These are text files with R code that you can save and run as needed. Scripts are useful for documenting your workflow and making analyses reproducible. Notebooks: R Notebooks combine code, output, and markdown text in one document. They are great for creating interactive reports and combining narrative with code. Figure 2.2: Example R Script Figure 2.3: Example R Notebook with Text, Code, and Output Areas Labelled 2.2 Introduction to R Packages and Installing Key Packages R comes with a robust set of core functions that allow you to perform a wide range of statistical analyses and data manipulations. However, the true power of R lies in its extensive ecosystem of packages, which extend its functionality far beyond the basics. These packages, developed by the global R community, enable users to perform specialised tasks, from advanced statistical modelling and machine learning to data visualisation and spatial analysis. Understanding how to install and use these packages is key to unlocking the full potential of R. 2.2.1 Introduction to R Packages A package in R is a collection of functions, data, and documentation that extends the base functionality of R. There are thousands of packages available, each designed to solve specific problems or add capabilities to your R environment. Using packages allows you to leverage the work of other developers and statisticians, saving you time and effort. 2.2.2 Installing and Loading Packages To use a package in R, you first need to install it, and then load it into your current R session. Packages are typically installed from the Comprehensive R Archive Network (CRAN), which is the primary repository for R packages. Installing a Package To install a package, use the install.packages() function. For example, to install the ggplot2 package, which is used for creating visualisations, you would run: install.packages(&quot;ggplot2&quot;) This command downloads the package from CRAN and installs it on your system. You only need to install a package once on your machine. Loading a Package Once a package is installed, you need to load it into your current R session using the library() function. For example: library(ggplot2) After loading the package, you can use its functions in your R code. Installing Multiple Packages at Once You can install multiple packages at once by passing a vector (we will come onto these later!) of package names to install.packages(): install.packages(c(&quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;tidyr&quot;)) This is useful when you’re setting up a new R environment or working on a project that requires several packages. 2.2.3 Key Packages for Data Analysis For those transitioning from SPSS to R, here are some key packages you’ll frequently use: tidyverse: A collection of packages for data manipulation, exploration, and visualisation. It includes packages like dplyr, ggplot2, tidyr, and readr. haven: Used to import and export SPSS, Stata, and SAS files. This package is essential for working with data originally created in SPSS. dplyr: A package for data manipulation. It provides functions for filtering, selecting, and transforming data, making it easier to manage datasets. ggplot2: A powerful package for creating complex visualisations. It is part of the tidyverse and is widely used for creating a variety of plots. readr: For importing and exporting CSV and other flat files. It provides functions that are faster and more flexible than base R’s equivalent functions. sf: If you are working with spatial data, sf is essential. It provides a simple feature (sf) framework for handling and analysing spatial data. survey: For survey data analysis. This package provides tools for analysing complex survey samples, including handling survey weights. psych: Useful for psychological research and other fields involving human behaviour. It provides tools for descriptive statistics, factor analysis, and more. 2.2.4 Managing Package Dependencies When working with projects, it’s important to manage package dependencies to ensure that your code runs smoothly on different systems. Consider using the renv package to manage project-specific dependencies. This package allows you to create isolated environments for your R projects, ensuring that the correct versions of packages are used. install.packages(&quot;renv&quot;) renv::init() 2.2.5 Summary R packages are a cornerstone of efficient and effective data analysis in R. Understanding how to install and load packages, as well as identifying key packages for your analysis, will significantly enhance your ability to work with data in R. By mastering packages, you unlock the full potential of R and make your workflow more powerful and streamlined. Throughout this book we will utilise a number of packages so make sure you are familiar with installing and loading different packages. 2.3 Coding Conventions and Best Practices In this section we briefly touch on coding conventions and best practices. However, we recommend you check out our other book on the Best Coding Practices in R. 2.3.1 Writing Clean and Readable Code Good coding practices enhance readability and maintainability. Here are some guidelines: Use Descriptive Names: Choose meaningful variable and function names that describe their purpose. # Good Variable Name Example total_boroughs &lt;- 32 # Bad Variable Name Example x &lt;- 1000 Consistent Indentation: Indent your code consistently to improve readability. # Good if (x &gt; 10) { y &lt;- x + 1 } # Bad if (x &gt; 10) { y &lt;- x + 1 } 2.3.2 Commenting and Structuring Scripts Commenting: Add comments to explain complex code or to outline the purpose of each section. # Calculate Total Number of Crimes total_arrests &lt;- sum(incident_data$number_arrests) Structuring Scripts: Organize your scripts into sections with comments or use RStudio’s built-in sectioning feature. # REQUIRED LIBRARIES ------------------------------------------------------ # Load necessary libraries library(dplyr) # DATA WRANGLING ---------------------------------------------------------- # Data preparation data &lt;- read.csv(&quot;data.csv&quot;) 2.4 Data Types and Structures 2.4.1 Introduction to Vectors, Data Frames, Lists, and Factors Vectors: A sequence of elements of the same type. Examples include numeric, character, and logical vectors. # Numeric vector numbers &lt;- c(1, 2, 3, 4, 5) # Character vector names &lt;- c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;) Data Frames: Two-dimensional tables where each column can be of a different type. Data frames are similar to SPSS datasets. # Creating a data frame df_offenders &lt;- data.frame(OffenderName = c(&quot;Alice&quot;, &quot;Bob&quot;), OffenderAge = c(25, 30)) Lists: Collections of objects that can be of different types, including other lists. # Creating a list my_list &lt;- list(Name = &quot;Alice&quot;, Age = 25, Scores = c(90, 85, 88)) Factors: Used for categorical data. They store both the values and the corresponding levels. # Creating a factor gender &lt;- factor(c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;)) 2.4.2 Comparing R Data Types to SPSS Data Types In SPSS: Variables can be numeric, string, or categorical. Data Structures include datasets, which are similar to R data frames. In R: Vectors are like SPSS variables. Data Frames are akin to SPSS datasets, with columns of different types. Lists provide more flexibility compared to SPSS’s data structures. Factors are used for categorical data, similar to SPSS’s categorical variables. 2.5 Basic Operations and Functions in R 2.5.1 Arithmetic Operations R performs basic arithmetic operations similarly to SPSS. Examples include addition, subtraction, multiplication, and division. # Arithmetic operations addition &lt;- 5 + 3 subtraction &lt;- 10 - 4 multiplication &lt;- 7 * 2 division &lt;- 8 / 2 exponentiation &lt;- 2^3 Exercise! Execute these operations in R and compare with results from SPSS’s Compute function. Can you do this in both the console and the script editor? 2.5.2 Logical Operations Logical operations include comparisons like equality and inequality, as well as logical operators such as AND and OR. # Logical operations equal &lt;- (5 == 5) # TRUE not_equal &lt;- (5 != 3) # TRUE greater_than &lt;- (5 &gt; 3) # TRUE and_operation &lt;- (5 &gt; 3 &amp; 4 &lt; 6) # TRUE or_operation &lt;- (5 &gt; 6 | 4 &lt; 6) # TRUE Exercise! Test these logical operations in R and compare them to SPSS’s logical operators. 2.5.3 Basic Functions R includes many built-in functions for statistical calculations, data manipulation, and more. Common functions include mean(), sd(), sum(), and length(). # Basic functions data &lt;- c(1, 2, 3, 4, 5) mean_value &lt;- mean(data) sd_value &lt;- sd(data) total_sum &lt;- sum(data) data_length &lt;- length(data) Exercise! Between January 2023 and December 2023 the number of Violent and Sexual Offences in Wandsworth were as follows: 568, 568, 603, 604, 685, 871, 697, 608, 657, 681, 630, and 720. (1) Create a vector representing the number of Violent and Sexual Offences in Wandsworth. (2) What was the mean number of crimes each month? (3) What was the total number of crimes across the year? (4) What is the length of the vector? 2.6 Conclusion In this chapter, you’ve learned about the RStudio interface, how to import data from SPSS, coding conventions, and the basics of R data types, structures, and operations. With this foundation, you’re prepared to start working with data in R and leveraging its powerful capabilities for analysis. In the next chapter, we’ll delve into more advanced data manipulation techniques to further enhance your skills. "],["data-management-in-r.html", "3 Data Management in R 3.1 Data Import and Export 3.2 Data Cleaning and Preparation 3.3 Working with Categorical Data 3.4 Conclusion", " 3 Data Management in R 3.1 Data Import and Export Effective data management is crucial for any analysis. In this section, we’ll explore how to import data from various formats, such as SPSS, Excel, and CSV files, and how to export your cleaned and prepared data to different formats. We will also compare the R methods to their SPSS equivalents. 3.1.1 Importing Data R offers several packages to import data from different sources: SPSS Files: Use the haven package to import SPSS data files (.sav). # Load the haven package library(haven) # Import SPSS data file data &lt;- read_sav(&quot;path_to_your_file.sav&quot;) SPSS Equivalent: In SPSS, you would use the File &gt; Open &gt; Data menu or the GET FILE command. Excel Files: Use the readxl package to import Excel files (.xlsx). # Load the readxl package library(readxl) # Import Excel file data &lt;- read_excel(&quot;path_to_your_file.xlsx&quot;) SPSS Equivalent: You would typically use File &gt; Open &gt; Data with Excel files in SPSS. CSV Files: Use base R’s read.csv() function or the readr package for more advanced options. # Base R method to import CSV file data &lt;- read.csv(&quot;path_to_your_file.csv&quot;) SPSS Equivalent: In SPSS, importing CSV files can be done via File &gt; Open &gt; Data or with the GET DATA command. 3.1.2 Exporting Data After cleaning and analysing your data, you may need to export it to various formats: To SPSS Format: # Export data to SPSS format write_sav(data, &quot;path_to_your_file.sav&quot;) SPSS Equivalent: Use File &gt; Save As in SPSS to export data. To Excel Format: # Load the openxlsx package library(openxlsx) #Export data to Excel format using openxlsx package write.xlsx(data, &quot;path_to_your_file.xlsx&quot;) SPSS Equivalent: Use File &gt; Save As in SPSS with Excel files. To CSV Format: # Export data to CSV format write.csv(data, &quot;path_to_your_file.csv&quot;, row.names = FALSE) SPSS Equivalent: In SPSS, you can use File &gt; Save As to save data as a CSV file. Exercise! Go to https://data.police.uk/data/ and download a CSV file of Metropolitan Police Service data for January 2024. Load the CSV file into a R dataframe called data_mps. How many observations and variables are there? Hint: You can find this in the environment pane. 3.2 Data Cleaning and Preparation Data cleaning is an essential step in preparing your data for analysis. This section covers handling missing data, recoding variables, filtering, subsetting, and performing data transformations. We’ll introduce the dplyr package, which is one of the most popular R packages for data manipulation. 3.2.1 Handling Missing Data Identify Missing Data: We can find missing data by looking for not available (NA) information. The sum command is used to give us an indication of how many missing pieces of data there are. # Check for missing data sum(is.na(data_mps)) # Check for missing data in a specific column sum(is.na(data_mps$Month)) SPSS Equivalent: Use the MISSING VALUES command or the Data &gt; Define Missing Values menu. Handling Missing Data: There are numerous approaches for handling missing data. Here we will cover the simplest methods. # Remove any rows with missing data data_mps_clean &lt;- na.omit(data_mps) # Remove any columns with missing data data_mps_clean &lt;- data_mps[, colSums(is.na(data_mps)) == 0] # Replace missing values with a specific value, such as 0. data_mps[is.na(data_mps)] &lt;- 0 #Replace missing values with a column value (e.g. Mode, Mean, ...) data_mps$Latitude[is.na(data_mps$Latitude)] &lt;- mean(data_mps$Latitude, na.rm = TRUE) SPSS Equivalent: Use the RECODE command or the Transform &gt; Replace Missing Values function. Exercise! Some of the commands above are composite commands encompassing multiple functions. Can you break them down to understand what each part of the function is doing? Exercise! In the examples above, in the second example we replaced all missing values with 0. We also replaced any missing latitudes with the average latitude across the dataset in the fourth example. Were these the best imputation choices? What method would you have chosen for dealing with outliers? 3.2.2 Filtering and Subsetting Data Filtering and subsetting data allow you to focus on specific observations of interest. Filtering Data in R: # Filter data where a variable equals a specific value data_filtered &lt;- filter(data, variable == &quot;value&quot;) SPSS Equivalent: Use the SELECT IF command… SELECT IF (variable = value). Subsetting Data in R: # Subset columns in R data_subset &lt;- select(data, column1, column2) SPSS Equivalent: Use the MATCH FILES / KEEP command or Data &gt; Select Cases. Exercise! The Context column is full of NA values. Can you subset the dataset to remove this column? Can you then filter the dataset to create a new dataframe called data_mps_drugs containing only crimes with the Crime Type recorded as ‘Drugs’. Hint: You do not need to list all of the columns you want to keep. Can you find a way to mention only the colum you want to drop? 3.2.3 Data Transformations Data transformations involve creating new variables or modifying existing ones. This can include mathematical operations, merging datasets, and more. Creating New Variables: # Create a new variable data &lt;- data %&gt;% mutate(new_variable = old_variable1 + old_variable2) SPSS Equivalent: Use the COMPUTE command: COMPUTE new_variable = old_variable1 + old_variable2. Merging Datasets: # Merge two datasets by a common key merged_data &lt;- merge(data1, data2, by = &quot;common_column&quot;) SPSS Equivalent: Use the MATCH FILES command. Exercise! How would you merge two datasets if they didn’t have a common key? Hint: Google is your friend! 3.2.4 The dplyr Pipeline You may have noticed in the previous section that we snuck in a funny looking bit of code – the pipeline operator, %&gt;% – when creating a new variable. The dplyr package uses the pipeline operator %&gt;% to streamline the process of applying a sequence of operations to data frames. The %&gt;% operator allows you to pass the result of one function directly into the next function, creating a clear and readable chain of commands. Pipe Data: The object on the left side of %&gt;% is passed as the first argument to the function on the right side. data %&gt;% function() Chaining Multiple Operations: You can chain multiple functions together, where the output of each function becomes the input for the next. This allows you to perform multiple operations on your data frame without needing to create intermediate variables. data %&gt;% function1() %&gt;% function2() %&gt;% function3() Benefits Readability: Makes code more readable by laying out operations in a clear, top-down sequence. Efficiency: Reduces the need for temporary variables or nested function calls. Exercise! Using the original data_mps dataframe can you recreate the data_mps_drugs dataframe using the dplyr pipeline operator with the relevant select and filter commands? 3.3 Working with Categorical Data Categorical data, or factors in R, are a common type of data in survey analysis. Understanding how to manipulate and analyse this type of data is essential. 3.3.1 Creating and Manipulating Factors Factors in R are used to represent categorical data and can be ordered or unordered. Here’s how to create and manipulate them: Creating a Factor: When creating a categorical factor variable you can manually specify the levels to provide an ordering or you can leave it blank to produce an unordered categorical variable. # Create a factor variable data$factor_variable &lt;- factor(data$categorical_variable, levels = c(&quot;Category 1&quot;, &quot;Category 2&quot;, &quot;Category 3&quot;)) SPSS Equivalent: This is similar to defining value labels and assigning them in SPSS. Reordering Factor Levels: You can reorder the factor levels by respecifying the ordering. # Reorder levels of a factor data$factor_variable &lt;- factor(data$factor_variable, levels = c(&quot;Category 3&quot;, &quot;Category 1&quot;, &quot;Category 2&quot;)) SPSS Equivalent: Use the VARIABLE LEVELS command. 3.3.2 Recoding Variables Recoding variables is a common task, such as transforming a continuous variable into a categorical one. Recoding Variables in R: #Load the dplyr library library(dplyr) # Recoding a variable using dplyr data &lt;- data %&gt;% mutate(new_variable = recode(old_variable, &quot;1&quot; = &quot;Category 1&quot;, &quot;2&quot; = &quot;Category 2&quot;)) SPSS Equivalent: Use the RECODE command… RECODE old_variable (1=&#39;Category 1&#39;) (2=&#39;Category 2&#39;) INTO new_variable. Exercise! Using the unfiltered data_mps dataset. Convert the Crime Type columns into a factor. 3.3.3 Frequency Tables and Cross-Tabulations Understanding the distribution of categorical data is often done through frequency tables and cross-tabulations. Frequency Tables: # Create a frequency table table(data$factor_variable) SPSS Equivalent: Use the FREQUENCIES command: FREQUENCIES VARIABLES=factor_variable. Cross-Tabulations: # Create a cross-tabulation table(data$factor_variable1, data$factor_variable2) SPSS Equivalent: Use the CROSSTABS command: CROSSTABS /TABLES=factor_variable1 BY factor_variable2. Exercise! Convert the Last Outcome Category to a categorical variable and view a cross-tabulation of the outcomes with the crime types. Which Crime Type has the highest number of Complete Investigations Where No Suspect Was Identified? 3.4 Conclusion In this chapter, we’ve covered essential data management tasks in R, including importing and exporting data, cleaning and preparing data, and working with categorical data. By comparing these processes with their SPSS equivalents, you can see how R can effectively replace or complement SPSS in your analytical workflow. In the next chapter, we’ll delve into descriptive statistics and data visualizations in R, continuing to build your skills in this versatile programming language. "],["connecting-to-and-accessing-a-postgresql-database.html", "4 Connecting to and Accessing a PostgreSQL Database 4.1 Introduction 4.2 Setting Up the Environment 4.3 Connecting to PostgreSQL (Using RPostgres) 4.4 Querying Data from PostgreSQL 4.5 Handling Errors and Troubleshooting 4.6 Conclusion", " 4 Connecting to and Accessing a PostgreSQL Database 4.1 Introduction Connecting to a PostgreSQL database from R allows you to leverage R’s powerful data analysis capabilities directly on your database’s data. This chapter will walk you through the steps required to set up a connection, query the database, and perform basic data operations. By the end of this chapter, you’ll be able to connect to your PostgreSQL database, retrieve data, and interact with it using R. 4.2 Setting Up the Environment 4.2.1 Installing Necessary Packages To connect to a PostgreSQL database, you need the RPostgres package, which provides an interface to PostgreSQL databases. Install the RPostgreSQL package: #Install the RPostgres Package install.packages(&quot;RPostgres&quot;) Load the RPostgreSQL package: #Load the RPostgres Package library(RPostgres) 4.3 Connecting to PostgreSQL (Using RPostgres) 1. Set up the connection: # Load the RPostgres package library(RPostgres) # Create a connection object con &lt;- dbConnect( Postgres(), dbname = &quot;your_database_name&quot;, host = &quot;your_host_address&quot;, port = 5432, user = &quot;your_username&quot;, password = &quot;your_password&quot; ) 2. Check the connection: I have connected to a test database (connection details not included for obvious reasons!). Running the following commands fetches a list of tables located on the database and prints the name of each. # Test the connection by listing tables tables &lt;- dbListTables(con) #Print the tables print(tables) ## character(0) 3. Close the connection: You should close the connection when you are done working with the database to free up resources and prevent potential data corruption. dbDisconnect(con) 4.4 Querying Data from PostgreSQL Once connected, you can execute SQL queries to retrieve and manipulate data. This guide does not cover the use of Structured Query Language (SQL) and the remainder of this chapter assumes a basic level of knowledge. 4.4.1 Executing a Query 1. Retrieve data: #Define a SQL query to select the first ten rows from all columns of the demographics database query &lt;- &quot;SELECT * FROM demographics LIMIT 10;&quot; # Execute the query to fetch data data_demogarphics &lt;- dbGetQuery(con, query) # View the retrieved data head(data_demogarphics) 2. Perform data operations: You can use SQL queries to perform operations such as filtering, aggregating, and joining data. # Example query with filtering query &lt;- &quot;SELECT column1, column2 FROM your_table_name WHERE column1 &gt; 100;&quot; filtered_data &lt;- dbGetQuery(con, query) # Example query with aggregation query &lt;- &quot;SELECT column1, AVG(column2) FROM your_table_name GROUP BY column1;&quot; aggregated_data &lt;- dbGetQuery(con, query) 4.5 Handling Errors and Troubleshooting When working with databases, you might encounter errors. Here are a few tips for troubleshooting: Check Connection Details: Ensure that your database name, host, port, username, and password are correct. Review SQL Queries: Make sure your SQL syntax is correct. SQL errors can prevent data retrieval or manipulation. Verify Permissions: Ensure that the user account has the necessary permissions for the operations you’re trying to perform. (Add screenshot here showing a common error message and how to troubleshoot it) 4.6 Conclusion In this chapter, you’ve learned how to connect to a PostgreSQL database from R, execute queries to retrieve and manipulate data, and write data back to the database. You’ve also learned some basic troubleshooting techniques. This foundational knowledge will allow you to leverage the power of R in conjunction with PostgreSQL to perform complex data analyses and manage your data more effectively. In the next chapter, we’ll explore more advanced data manipulation and analysis techniques using R. (Add a final screenshot or image reinforcing successful database connections and data operations) "],["descriptive-statistics-and-visualisations.html", "5 Descriptive Statistics and Visualisations 5.1 Introduction to Descriptive Statistics 5.2 Creating Visualisations with ggplot2 5.3 Descriptive Statistics with dplyr 5.4 Advanced Visualisation Techniques 5.5 Conclusion", " 5 Descriptive Statistics and Visualisations 5.1 Introduction to Descriptive Statistics Descriptive statistics provide a summary of the central tendency, dispersion, and shape of a dataset’s distribution. In SPSS, you may have used functions like FREQUENCIES or DESCRIPTIVES. In R, these tasks are just as straightforward, with added flexibility and power. In this chapter we will utilise a fictional dataset called police_activity_data. You can download a copy by clicking on the link. #Load the dataset from csv file police_activity_data &lt;- read.csv(&#39;data/police_activity_data.csv&#39;) #Explore the first 5 entries head(police_activity_data, 5) ## IncidentID Date Time IncidentType ResponseTime OfficersInvolved Outcome Borough IncidentSeverity ## 1 INC001 2024-08-31 15:32 Burglary 13 1 No Action North 1 ## 2 INC002 2024-08-15 19:15 Public Disturbance 12 4 Arrest East 2 ## 3 INC003 2024-08-19 07:27 Public Disturbance 7 2 Arrest West 5 ## 4 INC004 2024-08-14 02:48 Traffic Stop 14 2 Warning South 2 ## 5 INC005 2024-08-03 03:11 Traffic Stop 13 4 Warning East 5 5.1.1 Understanding Descriptive Statistics Measures of Central Tendency: These describe the center of the data (mean, median, mode). Measures of Dispersion: These describe the spread of the data (range, variance, standard deviation, interquartile range). Shape of Distribution: Skewness and kurtosis help describe the shape of the data distribution. 5.1.2 Basic Descriptive Statistics in R R provides multiple ways to compute descriptive statistics. Here are some basic functions. The summary() function provides a summary of each variable in a dataset. # Basic summary of all variables in a data frame summary(police_activity_data) ## IncidentID Date Time IncidentType ResponseTime OfficersInvolved Outcome Borough IncidentSeverity ## Length:200 Length:200 Length:200 Length:200 Min. : 5.00 Min. :1.00 Length:200 Length:200 Min. :1.00 ## Class :character Class :character Class :character Class :character 1st Qu.: 9.00 1st Qu.:2.00 Class :character Class :character 1st Qu.:2.00 ## Mode :character Mode :character Mode :character Mode :character Median :13.00 Median :3.00 Mode :character Mode :character Median :3.00 ## Mean :12.42 Mean :3.03 Mean :2.98 ## 3rd Qu.:16.00 3rd Qu.:4.00 3rd Qu.:4.00 ## Max. :20.00 Max. :5.00 Max. :5.00 But you can also use specific functions such as mean(), median() and sd() to calculate specific statistics. Note the usage of the na.rm = TRUE argument which tells R to ignore NA (missing) values. # Mean of a numeric variable mean(police_activity_data$ResponseTime, na.rm = TRUE) ## [1] 12.42 # Median of a numeric variable median(police_activity_data$ResponseTime, na.rm = TRUE) ## [1] 13 # Standard deviation of a numeric variable sd(police_activity_data$ResponseTime, na.rm = TRUE) ## [1] 4.290178 Exercise! Download the police_activity_data.csv file and load it into a R data frame. Produce a basic summary of the Response Time variable. What does the difference between the Mean and Median measure tell you about the skewness of the data? 5.2 Creating Visualisations with ggplot2 Visualisations are key to understanding and presenting data. The ggplot2 package is a powerful tool for creating a wide variety of plots, from simple bar charts to complex multi-layered visualisations. 5.2.1 Introduction to ggplot2 ggplot2 is part of the tidyverse collection of packages and is based on the grammar of graphics. The basic structure of a ggplot2 plot involves: Data: The dataset being used. Aesthetics (aes): Mapping variables to visual properties like x, y, color, size. Geometries (geom): The type of plot (e.g., geom_bar for bar charts, geom_point for scatter plots). Install ggplot2 if you haven’t already: install.packages(&quot;ggplot2&quot;) 5.2.2 Creating Basic Plots Here’s how you can create some basic plots in R using ggplot2: 5.2.2.1 Bar Charts Bar charts are used to display the frequency of categorical data. aes(x = categorical_variable): Maps the categorical variable to the x-axis. geom_bar(): Creates the bar chart. #Load the ggplot2 library library(ggplot2) # Bar chart for a categorical variable ggplot(your_data, aes(x = categorical_variable)) + geom_bar() + labs(title = &quot;Bar Chart of Categorical Variable&quot;, x = &quot;Category&quot;, y = &quot;Count&quot;) Exercise! Create a Bar Chart of the Borough variable in the police_activity_data dataset. Which borough has the greatest number of crimes? 5.2.2.2 Histograms Histograms show the distribution of a continuous variable. geom_histogram(binwidth = 10): Creates the histogram with specified bin width. fill and color: Customize the appearance. ggplot(your_data, aes(x = continuous_variable)) + geom_histogram(binwidth = 10, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = &quot;Histogram of Continuous Variable&quot;, x = &quot;Value&quot;, y = &quot;Frequency&quot;) Exercise! Create a Histogram of the ResponseTime variable in the police_activity_data dataset setting the bin size to 4. How is the response time distributed? 5.2.2.3 Boxplots Boxplots display the distribution of a variable and its potential outliers. aes(x = factor_variable, y = continuous_variable): Maps the factor variable to the x-axis and the continuous variable to the y-axis. geom_boxplot(): Creates the boxplot. ggplot(your_data, aes(x = factor_variable, y = continuous_variable)) + geom_boxplot() + labs(title = &quot;Boxplot of Continuous Variable by Factor&quot;, x = &quot;Factor&quot;, y = &quot;Value&quot;) Exercise! Create a Boxplot of the ResponseTime variable for each of the Borough in the police_activity_data dataset. Which Borough has the lowest Median response time? Which Borough has the smallest range of response times? 5.2.3 Customising Your Plots One of the strengths of ggplot2 is its flexibility in customising plots.You can add additional commands and features using the + notation. Adding Titles, Labels, and Themes labs(): Adds titles and axis labels. theme_minimal(): Applies a clean, minimalistic theme to the plot. ggplot(your_data, aes(x = categorical_variable)) + geom_bar(fill = &quot;lightblue&quot;, color = &quot;black&quot;) + labs(title = &quot;Bar Chart of Categorical Variable&quot;, x = &quot;Category&quot;, y = &quot;Count&quot;) + theme_minimal() Using Colors to Enhance Visualisations You can differentiate categories or highlight data points using color. scale_fill_brewer(palette = \"Pastel1\"): Applies a color palette to the fill of the boxplots. ggplot(your_data, aes(x = factor_variable, y = continuous_variable, fill = factor_variable)) + geom_boxplot() + labs(title = &quot;Boxplot of Continuous Variable by Factor&quot;, x = &quot;Factor&quot;, y = &quot;Value&quot;) + scale_fill_brewer(palette = &quot;Pastel1&quot;) Exercise! Using your Boxplot Diagram of the ResponseTime variable for each of the Borough in the police_activity_data dataset. Add some colour! 5.3 Descriptive Statistics with dplyr dplyr is a powerful tool for data manipulation and is also useful for summarising data. It works well alongside ggplot2 for data exploration and visualization. 5.3.1 Using dplyr to Summarise Data You can summarise data by calculating various descriptive statistics for different groups. group_by(factor_variable): Groups the data by the specified factor variable. summarize(): Calculates the mean and standard deviation for each group. # Load the dplyr library library(dplyr) # Summarise data: mean and standard deviation by group summary_data &lt;- your_data %&gt;% group_by(factor_variable) %&gt;% summarize( mean_value = mean(continuous_variable, na.rm = TRUE), sd_value = sd(continuous_variable, na.rm = TRUE) ) Exercise! Using the police_activity_data dataset, calculate the mean and standard deviation of the ResponseTime based on the IncidentType. Which Incident Type had the greatest mean response time? 5.3.2 Combining dplyr with ggplot2 You can easily combine the power of dplyr and ggplot2 to create insightful visualisations. # Example: Create a summary and plot it summary_data &lt;- your_data %&gt;% group_by(factor_variable) %&gt;% summarize(sd_value = sd(continuous_variable, na.rm = TRUE)) ggplot(summary_data, aes(x = factor_variable, y = sd_value)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Standard Deviation of Continuous Variable by Factor&quot;, x = &quot;Factor&quot;, y = &quot;Standard Deviation&quot;) Exercise! Building on the previous exercise, create a plot of the mean ResponseTime based on the IncidentType. The factors along the x-axis should be sorted alphabetically. Can you try sorting these in ascending order of their mean? 5.4 Advanced Visualisation Techniques Once you’re comfortable with basic plots, ggplot2 offers many advanced features for more complex visualisations. 5.4.1 Faceting Faceting allows you to create multiple plots based on the values of one or more variables. facet_wrap(~factor_variable): Creates separate plots for each level of factor_variable. ggplot(your_data, aes(x = continuous_variable)) + geom_histogram(binwidth = 10) + facet_wrap(~factor_variable) + labs(title = &quot;Histogram Faceted by Factor&quot;) Exercise! Use the facet wrap functionality to create a series of histograms representing the IncidentSeverity across the four different boroughs using a binwidth of 3. How does the response time vary across the four different boroughs? 5.4.2 Combining Multiple Geoms You can layer multiple geometries to create complex plots. geom_point(): Adds a scatter plot. geom_smooth(method = \"lm\"): Adds a linear regression line without the confidence interval. ggplot(your_data, aes(x = continuous_variable, y = another_continuous_variable)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Scatter Plot with Regression Line&quot;, x = &quot;X Variable&quot;, y = &quot;Y Variable&quot;) Exercise! Using dplyr produce a count of the number of crimes that occurred on each day. Use this information to create a scatterplot with a regression line. What do you notice about the trend? Hint I: Use the dplyr pipeline to group the data in conjunction with the summarise(count = n()) function. Hint II: If you can’t see a trendline you may need to review your Date variable data type. 5.4.3 Saving Your Plots Once you’ve created a plot, you might want to save it for later use. ggsave(): Saves the last plot with specified dimensions. # Save the plot to a file ggsave(&quot;my_plot.png&quot;, width = 8, height = 6) 5.5 Conclusion In this chapter, we explored the fundamentals of descriptive statistics and data visualisation in R, tools essential for any data analysis, including crime analysis. Starting with basic summary statistics, such as measures of central tendency and dispersion, we demonstrated how to gain quick insights into your data. We then explored the power of visualisations, learning how to create bar plots, histograms, boxplots, and scatterplots using the ggplot2 package, one of R’s most versatile and widely-used visualization libraries. These techniques allow you to uncover patterns, trends, and potential outliers in your data, transforming raw numbers into visual stories that can be more easily interpreted and communicated. As you continue to work with crime data or any other datasets, these descriptive statistics and visualisation skills will form the foundation for more advanced analyses. Whether summarising crime rates across boroughs or visualising the distribution of incident response times, these tools will help you to turn data into actionable insights. In the next chapters, we will build upon these concepts, diving into inferential statistics and regression analysis, where you’ll learn to make predictions and draw conclusions beyond mere descriptions. The knowledge you’ve gained here will be crucial as we move forward, so make sure to revisit these techniques regularly as you become more familiar with R. "],["survey-analysis-in-r.html", "6 Survey Analysis in R 6.1 Introduction to Survey Data 6.2 Importing and Preparing Survey Data 6.3 Descriptive Analysis of Survey Data 6.4 Weighting Survey Data 6.5 Conclusion", " 6 Survey Analysis in R Survey data is a powerful tool in crime analysis, providing insights into public perceptions, victimisation rates, and more. In this chapter, we will explore how to manage and analyse survey data using R. For those familiar with SPSS, you’ll find that while the syntax and interface differ, R offers extensive capabilities for survey analysis, often with greater flexibility and control. 6.1 Introduction to Survey Data Survey data often come with complex structures, including weighting, stratification, and clustering, which need to be handled appropriately to ensure valid analysis. In SPSS, you may have used procedures like FREQUENCIES, CROSSTABS, and WEIGHT to analyse survey data. In R, you can accomplish these tasks using various packages, with survey and srvyr being among the most powerful for survey data. The haven package is also particularly useful for importing SPSS .sav files directly into R. Install the packages if you haven’t already: install.packages(&quot;survey&quot;) install.packages(&quot;srvyr&quot;) install.packages(&quot;haven&quot;) 6.1.1 Key Concepts in Survey Analysis Weighting: Adjusts the data to represent the population more accurately, compensating for oversampling or undersampling. Stratification: Divides the population into subgroups (strata) before sampling to ensure representation from each subgroup. Clustering: Groups the population into clusters, where a random sample of clusters is then selected. 6.1.2 Understanding Survey Data Structures Survey data typically consists of responses from individuals to a set of questions, often with demographic information included. In SPSS, survey data is usually stored in .sav files with each row representing a respondent and each column representing a variable (question, demographic information, etc.). In R, survey data is typically handled within a data frame, where the structure is similar. Survey datasets might also include: Categorical variables (e.g., gender, education level) Numerical variables (e.g., age, income) Weighting variables (to adjust for survey sampling) 6.2 Importing and Preparing Survey Data Just like SPSS, R can import survey data from various file formats, including SPSS files. Here’s how you can import SPSS survey data into R: # Load haven package library(haven) # Import SPSS data survey_data &lt;- read_sav(&quot;data/survey_data.sav&quot;) # View the first few rows of the data head(survey_data) #Produce a summary of the data summary(survey_data) Once imported, survey data in R can be manipulated just like any other data frame. You can use functions from base R or packages like dplyr to filter, select, and mutate your data, just as you might use similar functions in SPSS. 6.2.1 Converting Data for Survey Analysis Before we can analyse the data, we need to define the survey design. This step involves specifying the survey weights, strata, and clusters if applicable. The svydesign() command creates a survey design object that tells R how your data were sampled, which is crucial for accurate analysis. #Load the survey library library(survey) # Assuming &#39;weight&#39;, &#39;strata&#39;, and &#39;cluster&#39; are your variables survey_design &lt;- svydesign( id = ~cluster, strata = ~strata, weights = ~weight, data = survey_data ) 6.3 Descriptive Analysis of Survey Data With the survey design object created, you can now perform various descriptive analyses. Descriptive statistics are often the first step in analysing survey data. In SPSS, you might use Frequencies, Descriptives, or Crosstabs commands. In R, these can be replicated using functions from base R or more specialised packages like dplyr or janitor. 6.3.1 Calculating Means and Totals To calculate means, totals, or other statistics, you can use functions from the survey package. # Mean of a variable mean_variable_name &lt;- svymean(~variable_name, design = survey_design) mean_variable_name # Total population estimate total_variable_name&lt;- svytotal(~variable_name, design = survey_design) total_variable_name If applicable, these commands will return the weighted mean and total, respectively, adjusting for the survey’s design. 6.3.2 Frequencies and Cross-tabulations Frequencies and cross-tabulations are commonly used in survey analysis to summarise categorical variables. These tables are weighted according to the survey design, providing a more accurate reflection of the population. Frequencies To calculate the frequency of responses to a survey question (categorical variable), you can use the svytable() function. This provides more detailed summary statistics (similar to SPSS’s Frequencies output). # Frequency distribution svytable(~variable_name, design = survey_design) Cross-tabulations Cross-tabulations are used to examine the relationship between two categorical variables. In SPSS, this is done using the Crosstabs command. In R, you can use the table() function or the janitor package to create cross-tabulations: # Cross-tabulation of two categorical variables svytable(~variable_name_1 + variable_name_2, design = survey_design) For more detailed cross-tabulation (with proportions and totals) you can use the janitor package. # Load the janitor package library(janitor) # Cross-tabulation with janitor tabyl(survey_data, variable_name_1, variable_name_2) 6.3.3 Comparing Results with SPSS Survey Functions In SPSS, descriptive analysis is often performed using menu-driven commands with various output options. In R, you have more flexibility and control over the analysis and the output, though it may require writing more code. For example, while SPSS may offer a GUI-based approach to creating cross-tabulations with options for row/column percentages, R’s approach provides more control and customisation, often in fewer steps once the user is comfortable with the syntax. But most importantly R ensures that your analysis can be easily reproduced. Exercise! You have a dataset from a national survey on perceptions of crime. This dataset provides a comprehensive analysis of crime and policing by examining crime types, victim demographics, police responses, community engagement, and socioeconomic factors to understand the respondents crime experiences and perceptions. Download the Crime Survey Data (SPSS Format) and perform a descriptive analysis of the data. The steps you need to follow are listed below to help you. Hints: Load the dataset Create a survey design object Create a frequency distribution of the crime_type and gender variable. Cross-tabulate crime_type by region and gender (separately and together) to see the distribution across different regions and genders. Create a new variable to categorise satisfaction into high and low satisfaction (above 3 = High, 3 or below = Low). Update the survey design with the new variable. Calculate the mean number of community programs by satisfaction level and gender 6.4 Weighting Survey Data Weights are crucial in survey analysis to correct for biases introduced by the sampling design. 6.4.1 Applying Weights Survey data often includes a weight variable to adjust for the sampling design. This is particularly common in complex surveys where the probability of selection differs among respondents. In SPSS, weights are applied using the Weight Cases function. In R, you can apply weights directly in the survey design phase, as shown earlier. Here’s a reminder: survey_design &lt;- svydesign( id = ~cluster, strata = ~strata, weights = ~weight, data = survey_data ) 6.4.2 Analysing Weighted Survey Data Once weights are applied, all the subsequent analyses (means, totals, regressions) will automatically account for these weights, ensuring that your results are representative of the population. For example, to calculate weighted means or proportions: Weighted Means # Weighted mean of a numeric variable svymean(~variable_name, design = svy_design) Weighted Proportions # Weighted proportion for a categorical variable svytable(~variable_name, design = svy_design) Weighted Cross-tabulations # Weighted cross-tabulation svytable(~variable_name_1 + variable_name_2, design = svy_design) These functions ensure that your survey analysis correctly reflects the survey design and weights, providing more accurate estimates and inferences. 6.5 Conclusion In this chapter, we explored the basics of survey analysis in R, covering the import of SPSS survey data, applying weights, and performing basic descriptive statistics. The tools and techniques introduced here are powerful, enabling you to transition smoothly from SPSS to R while expanding your analytic capabilities. For practice, try importing your own survey data, defining the survey design, and performing some of the analyses shown in this chapter. As you become more comfortable with these processes in R, you’ll find it offers greater flexibility and control over your survey analyses than SPSS. "],["inferential-statistics.html", "7 Inferential Statistics 7.1 Hypothesis Testing 7.2 Correlation Analysis 7.3 Conclusion", " 7 Inferential Statistics In this chapter, we’ll explore the basics of inferential statistics, focusing on hypothesis testing and correlation analysis. We will walk through how to perform these tests in R, compare them to similar processes in SPSS, and understand how to interpret the results. 7.1 Hypothesis Testing Hypothesis testing is a fundamental aspect of inferential statistics, allowing you to draw conclusions about populations based on sample data. In R, common hypothesis tests like t-tests, chi-square tests, and ANOVA are straightforward to perform. Let’s look at each in detail. 7.1.1 T-tests A t-test is used to determine if there is a significant difference between the means of two groups. This test is equivalent to the COMPARE MEANS function in SPSS. 7.1.1.1 One-sample t-test A one-sample t-test compares the mean of a single group against a known value (e.g., a population mean). For example, let us look at the number of days each officer in the Serious Crime Unit has taken absence this year and compare it to the average number of days across all officers last year. We want to know if the mean differs from last year. # Example: One-sample t-test # Testing if the mean of a sample is significantly different from 50 data_officerabsencedays &lt;- c(48, 50, 52, 51, 49, 47, 53, 50, 52, 48) t.test(data_officerabsencedays, mu = 50) ## ## One Sample t-test ## ## data: data_officerabsencedays ## t = 0, df = 9, p-value = 1 ## alternative hypothesis: true mean is not equal to 50 ## 95 percent confidence interval: ## 48.56929 51.43071 ## sample estimates: ## mean of x ## 50 7.1.1.2 Independent two-sample t-test An independent t-test compares the means of two independent groups. For example, let us review the Stop and Search data for Merton and Kingston across a 6 month period. We want to know if the average number of stop and searches differs between the two boroughs. # Example: Independent two-sample t-test # Comparing scores of two independent groups data_stopsearch_merton &lt;- c(53, 55, 68, 65, 72, 63) data_Stopsearch_kingston &lt;- c(59, 69, 65, 70, 75, 67) t.test(data_stopsearch_merton, data_Stopsearch_kingston) ## ## Welch Two Sample t-test ## ## data: data_stopsearch_merton and data_Stopsearch_kingston ## t = -1.2967, df = 9.1156, p-value = 0.2266 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -13.249305 3.582638 ## sample estimates: ## mean of x mean of y ## 62.66667 67.50000 7.1.1.3 Paired t-test A paired t-test compares means from the same group at different times (e.g., before and after a treatment). # Example: Paired t-test # Comparing pre- and post-treatment scores for the same group pre_treatment &lt;- c(100, 102, 104, 106, 108) post_treatment &lt;- c(110, 111, 115, 117, 120) t.test(pre_treatment, post_treatment, paired = TRUE) ## ## Paired t-test ## ## data: pre_treatment and post_treatment ## t = -20.788, df = 4, p-value = 3.164e-05 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -12.015715 -9.184285 ## sample estimates: ## mean difference ## -10.6 7.1.1.4 Interpreting T-Test Results T-Value: Indicates the size of the difference relative to the variation in your sample data. P-Value: Tells you whether the observed difference is statistically significant. A p-value less than 0.05 typically indicates statistical significance. Confidence Interval: Provides a range within which the true population parameter is likely to fall. In SPSS, p-values and confidence intervals are found in the output tables after running the analysis. In R, they appear in the results from the t.test() function. Exercise! The Metropolitan Police Service have implemented a new strategy to target Violence Against Women and Girls in 5 wards. The number of incidents in March across these 5 wards was 100, 102, 104, 106, and 108. After the implementation of the new strategy the number of incidents in April across these 5 wards was 110, 111, 115, 117, 120 respectively. Has the new strategy had an impact on the number of incidents? 7.1.2 Chi-square Tests Chi-square tests assess the relationship between categorical variables. In SPSS, this corresponds to the CROSSTABS function with the “Chi-square” option. 7.1.2.1 Chi-Square Test of Independence The Chi-Square Test of Independence tests if two categorical variables are independent. For example, you might want to determine if there is an association between the type of crime and the borough where the crime occurred. # Example: Chi-square test # Testing the association between two categorical variables # Example data: Crime frequencies in different boroughs crime_data &lt;- matrix(c( 100, 50, 30, # Borough 1: Crime Type A, B, C 80, 40, 20, # Borough 2: Crime Type A, B, C 70, 30, 25 # Borough 3: Crime Type A, B, C ), nrow = 3, byrow = TRUE) # Add row and column names rownames(crime_data) &lt;- c(&quot;Borough 1&quot;, &quot;Borough 2&quot;, &quot;Borough 3&quot;) colnames(crime_data) &lt;- c(&quot;Crime Type A&quot;, &quot;Crime Type B&quot;, &quot;Crime Type C&quot;) # Perform the Chi-Square Test of Independence chisq_test_independence &lt;- chisq.test(crime_data) # Print the results print(chisq_test_independence) ## ## Pearson&#39;s Chi-squared test ## ## data: crime_data ## X-squared = 1.9076, df = 4, p-value = 0.7527 7.1.2.2 Chi-Square Goodness-of-Fit Test: The Chi-Square Goodness-of-Fit Test tests if a single categorical variable follows a specific distribution. For example, suppose you have observed the frequency of crimes across different types, and you want to test if these frequencies are uniformly distributed. # Example: Chi-square test # Testing the distribution of a categorical variable # Example data: Observed frequencies of crime types observed_frequencies &lt;- c(100, 150, 80, 70) # Frequencies of Crime Type A, B, C, D # Expected frequencies under the null hypothesis (uniform distribution) expected_frequencies &lt;- rep(sum(observed_frequencies) / length(observed_frequencies), length(observed_frequencies)) # Perform the Chi-Square Goodness-of-Fit Test chisq_test_goodness_of_fit &lt;- chisq.test(observed_frequencies, p = rep(1/length(observed_frequencies), length(observed_frequencies))) # Print the results print(chisq_test_goodness_of_fit) ## ## Chi-squared test for given probabilities ## ## data: observed_frequencies ## X-squared = 38, df = 3, p-value = 2.826e-08 7.1.2.3 Interpreting Chi-Square Test Results Chi-Square Statistic: Measures the deviation of observed frequencies from expected frequencies. P-Value: Indicates whether the association or distribution is statistically significant. A p-value less than 0.05 suggests a significant result. Degrees of Freedom (df): The number of independent values in the test, affecting the chi-square distribution. In SPSS, p-values and confidence intervals are found in the output tables after running the analysis. In R, they appear in the results from the chisq.test() function. Exercise! You want to identify if the number of Stop and Search performed in the month of September across 6 Boroughs where the Stop and Search occurred is Uniformly distributed. Use the following data to identify if the Number of Stop and Search performed is uniform across the 6 Boroughs. Richmond: 36 // Kingston: 25 // Merton: 28 // Sutton: 34 // Croydon: 42 // Wandsworth: 32 7.1.3 ANOVA (Analysis of Variance) ANOVA tests are used to compare the means of three or more groups. This is analogous to the ONE-WAY ANOVA function in SPSS. For example, let us review the number of crimes reported in three geographic areas over a four week period. We will perform a one-way ANOVA to determine if there are significant differences in the average number of crimes reported across these districts. # Example: One-way ANOVA # Comparing scores across three different groups ward1 &lt;- c(150, 155, 160, 158) #Ward 1 ward2 &lt;- c(163, 165, 172, 174) #Ward 2 ward3 &lt;- c(178, 172, 183, 153) #Ward 3 data &lt;- data.frame( score = c(ward1, ward2, ward3), group = factor(rep(1:3, each = 4)) ) anova_result &lt;- aov(score ~ group, data = data) summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 559.5 279.75 3.822 0.0629 . ## Residuals 9 658.7 73.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.1.3.1 Interpreting ANOVA Results F-Value: Indicates the ratio of variance between the groups to the variance within the groups. A higher F-value suggests a greater likelihood that the group means are different. P-Value: Indicates whether the group means are significantly different. A p-value less than 0.05 usually suggests that there is a significant difference among group means. In SPSS, p-values and confidence intervals are found in the output tables after running the analysis. In R, they appear in the results from the summary() function for ANOVA. 7.1.3.2 Post-Hoc ANOVA Analysis (if significant) If the ANOVA test is significant, you may want to perform a post-hoc test to identify which specific groups differ from each other. A common post-hoc test is Tukey’s Honest Significant Difference (HSD) test. # Perform Tukey&#39;s HSD test for post-hoc analysis tukey_result &lt;- TukeyHSD(anova_result) # Print the results of Tukey&#39;s HSD test print(tukey_result) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = score ~ group, data = data) ## ## $group ## diff lwr upr p adj ## 2-1 12.75 -4.140416 29.64042 0.1431152 ## 3-1 15.75 -1.140416 32.64042 0.0670591 ## 3-2 3.00 -13.890416 19.89042 0.8750277 Interpreting Tukey’s HSD Results Pairwise Comparisons: Tukey’s HSD provides pairwise comparisons between all groups. Significant differences are indicated where the confidence intervals for the difference between group means do not contain zero. Adjusted p-Values: The test adjusts for multiple comparisons to control the family-wise error rate. Exercise! Local residents in 3 boroughs were asked to rate their confidence in the Metropolitan Police Service rating their scores from 0 to 10. The average score across a 3 month period is as follows. Kingston : 7.0, 7.5, 8.2 Richmond : 8.3, 6.4, 7.9 Sutton : 6.4, 4.5, 5.8 Does the mean confidence level across the 3 month preiod differ between the three boroughs? 7.2 Correlation Analysis Correlation analysis measures the strength and direction of the relationship between two variables. The most common methods are Pearson and Spearman correlations, which are available in both SPSS and R. 7.2.1 Pearson Correlation The Pearson correlation measures the linear relationship between two continuous variables. It’s equivalent to BIVARIATE CORRELATIONS in SPSS. For example, suppose you have data on the number of crimes reported and the number of police patrol hours in different beats and you want to see if there’s a linear relationship between these two variables. # Example: Pearson correlation # Measuring the correlation between two continuous variables # Create example data: Crime rate and police patrol hours data_beat &lt;- data.frame( patrol_hours = c(20, 30, 25, 40, 35, 45, 50, 60, 55, 65), # Hours of patrol crime_rate = c(15, 22, 18, 25, 20, 30, 28, 35, 32, 40) # Number of crimes ) # View the dataset print(data_beat) ## patrol_hours crime_rate ## 1 20 15 ## 2 30 22 ## 3 25 18 ## 4 40 25 ## 5 35 20 ## 6 45 30 ## 7 50 28 ## 8 60 35 ## 9 55 32 ## 10 65 40 cor(data_beat$patrol_hours, data_beat$crime_rate, method = &quot;pearson&quot;) ## [1] 0.9766032 Interpreting Pearson Correlation Correlation Coefficient (r): Ranges from -1 to 1. A value close to 1 indicates a strong positive correlation, while a value close to -1 indicates a strong negative correlation. A value around 0 suggests no linear correlation. P-Value: Tests if the observed correlation is significantly different from zero. In SPSS, the correlation coefficient and p-value are reported together in a table. In R, these can be accessed using the cor.test() function if you need detailed statistical outputs. 7.2.2 Spearman Correlation Spearman correlation is a non-parametric measure of rank correlation, useful when the data is not normally distributed or the relationship is not linear. We will use the same dataset to see if there is a monotonic relationship between crime rates and patrol hours. # Example: Spearman correlation # Measuring the correlation between two variables using ranks cor(data_beat$patrol_hours, data_beat$crime_rate, method = &quot;spearman&quot;) ## [1] 0.9757576 Interpreting Spearman Correlation Spearman’s Rank Correlation Coefficient (ρ): Ranges from -1 to 1, similar to Pearson. It measures how well the relationship between two variables can be described by a monotonic function. In SPSS, the correlation coefficient and p-value are reported together in a table. In R, these can be accessed using the cor.test() function if you need detailed statistical outputs. 7.2.3 Pearson vs Spearman? Pearson’s correlation coefficient is used when you want to measure the strength and direction of a linear relationship between two continuous variables that are normally distributed. It assesses how well the relationship between the variables can be described by a straight line. Use Pearson’s correlation when the data is interval or ratio and there is a linear relationship. Spearman’s rank correlation coefficient is suitable when the relationship between the variables is monotonic but not necessarily linear, or when the data does not meet the assumptions required for Pearson’s correlation, such as normality or interval scale. Spearman’s correlation assesses how well the relationship between the variables can be described by a monotonic function, which means it evaluates whether higher ranks in one variable correspond to higher ranks in another, regardless of the exact form of the relationship. In summary, use Pearson’s correlation for linear relationships with continuous data and Spearman’s correlation for monotonic relationships or when data is ordinal or not normally distributed. 7.2.4 Visualising Correlations Visualising correlations can help in understanding the relationship between multiple variables. The corrplot package in R provides a convenient way to create correlation matrices. Using the same beat dataset used above. # Example: Correlation matrix visualization # Creating and visualizing a correlation matrix library(corrplot) ## corrplot 0.94 loaded # Compute the correlation matrix cor_matrix &lt;- cor(data_beat) # Visualize the correlation matrix corrplot(cor_matrix, method = &quot;circle&quot;) Interpreting the Correlation Matrix Plot Colours and Sizes: Represent the strength and direction of the correlation. Positive correlations are typically shown in one colour and negative correlations in another. Magnitude of Correlation: Larger circles or stronger colours indicate stronger correlations. Exercise! You have data on the number of community outreach programs conducted in 6 boroughs as well as the associated crime rates. You want to determine if there’s a linear relationship between these two variables. Use the data below to perform a correlation analysis. Borough Community Programs Number of Crimes Kingston 4 145 Merton 5 154 Sutton 8 218 Croydon 19 255 Lambeth 17 234 Wandsworth 15 189 7.3 Conclusion In this chapter, you’ve learned how to perform common inferential statistical tests in R, including t-tests, chi-square tests, and ANOVA, as well as how to conduct and visualise correlation analyses. Each test has an equivalent function in SPSS, but R provides more flexibility and control over the analysis process. This chapter has laid the groundwork for applying these techniques to your own data analyses, building on your existing knowledge from SPSS. "],["regression-analysis-1.html", "8 Regression Analysis 8.1 Introduction to Regression Analysis 8.2 Simple Linear Regression in R 8.3 Multiple Linear Regression 8.4 Checking Model Assumptions 8.5 Transformations and Interaction Terms 8.6 Logistic Regression 8.7 Checking Model Assumptions for Logistic Regression 8.8 Model Validation and Diagnostics 8.9 Conclusion", " 8 Regression Analysis Regression analysis is one of the most commonly used statistical techniques for exploring relationships between variables. This chapter will walk you through the fundamentals of regression analysis in R, comparing it with similar procedures in SPSS. We will cover simple and multiple linear regression, logistic regression, and the necessary steps to validate your models thoroughly, including checking assumptions, transformations, and handling multicollinearity. 8.1 Introduction to Regression Analysis Regression analysis aims to model the relationship between a dependent variable (often referred to as the outcome or response variable) and one or more independent variables (predictors). The output of a regression model helps in understanding the influence of predictors on the outcome and can be used for prediction purposes. 8.1.1 What is Linear Regression? Linear regression models the relationship between a dependent variable \\(Y\\) and one or more independent variables \\(X\\). The simplest form is the linear equation: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\epsilon \\] Where: \\(Y\\) is the dependent variable. \\(\\beta_0\\) is the intercept. \\(\\beta_1\\) is the slope of the line (coefficient of \\(X_1\\)). \\(X_1\\) is the independent variable. \\(\\epsilon\\) is the error term, representing the difference between observed and predicted values. 8.2 Simple Linear Regression in R In this section we will utilise a fictional dataset on Violence Against Women &amp; Girls (VAWG). You can download a copy by clicking on the link. 8.2.1 Performing Simple Linear Regression Simple linear regression involves a single independent variable. In R, this is done using the lm() function. # Perform simple linear regression model &lt;- lm(y ~ x, data = data) # View the model summary summary(model) SPSS Equivalent: In SPSS, linear regression is performed through Analyse &gt; Regression &gt; Linear…. In this example, we will fit a linear model to predict the number of Reported_VAWG_Incidents in a given geographic area using the Unemployment_Rate. # Load the dataset from csv file data_vawg &lt;- read.csv(&#39;data/data_vawg.csv&#39;) # Perform simple linear regression model &lt;- lm(Reported_VAWG_Incidents ~ Unemployment_Rate, data = data_vawg) # View the model summary summary(model) ## ## Call: ## lm(formula = Reported_VAWG_Incidents ~ Unemployment_Rate, data = data_vawg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.883 -7.665 0.156 8.327 36.064 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 132.4752 4.3074 30.755 &lt; 2e-16 *** ## Unemployment_Rate 2.5977 0.4188 6.203 1.32e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.26 on 98 degrees of freedom ## Multiple R-squared: 0.282, Adjusted R-squared: 0.2746 ## F-statistic: 38.48 on 1 and 98 DF, p-value: 1.321e-08 8.2.2 Interpreting the Output When you run the summary(model) command, the following elements are crucial: Coefficients: These indicate the relationship between the independent variable (Unemployment_Rate) and the dependent variable (VAWG). A positive coefficient for Unemployment Rate suggests that as Unemployment Rate increases, the Number of VAWG Incidents also increases. R-squared: This statistic tells you the proportion of variance in the dependent variable that can be explained by the dependent variable. An R-squared close to 1 indicates a good fit. P-value: Associated with the coefficients, the p-value helps you determine whether the relationship between the variables is statistically significant. A p-value less than 0.05 typically indicates significance. We can see – indicated by the three ’***’ – that the relationship between unemployment and the number of incidents is statistically significant. SPSS Equivalent: SPSS outputs similar tables with coefficients, R-squared values, and significance levels. 8.3 Multiple Linear Regression Multiple linear regression extends simple linear regression by including more than one independent variable. 8.3.1 Performing Multiple Linear Regression Extending the model is easily completed by increasing the number of terms in the lm() function. # Perform multiple linear regression model_multiple &lt;- lm(y ~ x1 + x2 + x3, data = data) # View the model summary summary(model_multiple) In this example we will expand our model to predict the Number of Reported VAWG Incidents by increasing the number of independent variables. # Perform multiple linear regression model_multiple &lt;- lm(Reported_VAWG_Incidents ~ Unemployment_Rate + Community_Programs + Population_Density + Police_Patrols, data = data_vawg) # View the model summary summary(model_multiple) ## ## Call: ## lm(formula = Reported_VAWG_Incidents ~ Unemployment_Rate + Community_Programs + ## Population_Density + Police_Patrols, data = data_vawg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.8934 -5.8510 0.5094 6.1948 26.8093 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 156.337257 9.142655 17.100 &lt; 2e-16 *** ## Unemployment_Rate 2.543248 0.359892 7.067 2.64e-10 *** ## Community_Programs -2.155397 0.350556 -6.149 1.83e-08 *** ## Population_Density 0.001013 0.002283 0.443 0.658 ## Police_Patrols -0.057337 0.116747 -0.491 0.624 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.51 on 95 degrees of freedom ## Multiple R-squared: 0.4886, Adjusted R-squared: 0.4671 ## F-statistic: 22.69 on 4 and 95 DF, p-value: 3.545e-13 8.3.2 Detailed Interpretation of the Output Coefficients: Each coefficient represents the effect of a one-unit change in the predictor while holding the other variables constant. For instance, the coefficient for Community_Programs indicates how much the Reported_VAWG_Incidents decreases (or increases with a positive coefficient) with an increase in the number of Community_Programs, assuming the other variables remain unchanged. R-squared and Adjusted R-squared: R-squared indicates the proportion of variance explained by all predictors. Adjusted R-squared adjusts for the number of predictors in the model, providing a more accurate measure when comparing models with different numbers of predictors. P-values: Check these for each predictor to determine if they significantly contribute to the model. 8.4 Checking Model Assumptions It’s crucial to check if the assumptions of linear regression are met before trusting the results. This section runs through how to check the various assumptions. SPSS Equivalent: SPSS provides diagnostics to check these assumptions under the Regression dialog, though some assumptions like multicollinearity (VIF) require manual checks through syntax or additional output. 8.4.1 Assumption 1: Linearity The relationship between the predictors and the outcome should be linear. How to Check: Plot residuals versus fitted values. If the assumption is met, the points should scatter randomly around the horizontal line (y = 0). #Plot residuals vs fitted values plot(model_multiple$fitted.values, resid(model_multiple)) #Add horizontal line at y = 0 abline(h = 0, col = &quot;red&quot;) 8.4.2 Assumption 2: Normality of Residuals Residuals (errors) should be normally distributed. How to Check: Create a Q-Q plot. Residuals should closely follow the 45-degree line. #Create a Q-Q plot qqnorm(resid(model_multiple)) #Add Q-Q line qqline(resid(model_multiple), col = &quot;red&quot;) 8.4.3 Assumption 3: Homoscedasticity Residuals should have constant variance across all levels of the independent variables. How to Check: The Scale-Location plot helps diagnose this. Look for a horizontal line with equally spread points. #Plot Scale-Location plot plot(model_multiple, which = 3) 8.4.4 Assumption 4: Independence of Errors Errors should be independent of each other. How to Check: The Durbin-Watson test can be performed using the car package. #Load the car library library(car) ## Loading required package: carData #Run the DurbinWatson test durbinWatsonTest(model_multiple) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.2217769 2.415587 0.03 ## Alternative hypothesis: rho != 0 8.4.5 Assumption 5: Multicollinearity Predictors should not be too highly correlated with each other, as this can inflate standard errors and make it difficult to determine the effect of each predictor. How to Check: The Variance Inflation Factor (VIF) helps in detecting multicollinearity. VIF values greater than 10 may indicate problematic multicollinearity. #Calculate the Variance Inflation Factors vif(model_multiple) ## Unemployment_Rate Community_Programs Population_Density Police_Patrols ## 1.005358 1.005636 1.020414 1.021191 8.5 Transformations and Interaction Terms 8.5.1 When and How to Apply Transformations Transformations can help correct violations of assumptions. For example: Log Transformation: Useful when the relationship between the variables is exponential rather than linear. #Linear Model with Log Transformation model_log &lt;- lm(log(y) ~ x1 + x2 + x3, data = data) summary(model_log) Square Root Transformation: Applied to right-skewed data to stabilise variance. #Linear Model with Square Root Transformation model_sqrt &lt;- lm(sqrt(y) ~ x1 + x2 + x3, data = data) summary(model_sqrt) 8.5.2 Using Interaction Terms Interaction terms allow you to model the combined effect of two or more predictors. #Create Linear Model with Interaction Term model_interaction &lt;- lm(y ~ x1 * x2, data = data) summary(model_interaction) SPSS Equivalent: SPSS allows adding interaction terms through manual creation or using the Paste option to modify syntax. For instance, the effect of Unemployment_Rate on Reported_VAWG_Incidents might depend on the level of Population_Density. Let us include an interaction term between Unemployment_Rate and Population_Density. #Create Linear Model with Interaction Term model_interaction &lt;- lm(Reported_VAWG_Incidents ~ Unemployment_Rate * Population_Density, data = data) summary(model_interaction) The interaction term (Unemployment_Rate:Population_Density) allows us to see how the relationship between Unemployment_Rate and Reported_VAWG_Incidents changes at different levels of Population_Density. Exercise! Residential burglaries are on the up and you want to assess which factors are contributing to the increase. Using the following dataset data_burglaries perform a multiple linear regression. 8.6 Logistic Regression When the dependent variable is binary (e.g., Yes/No, 0/1), logistic regression is appropriate. In this section we will utilise a fictional dataset on Police Arrests. You can download a copy by clicking on the link. 8.6.1 Performing Logistic Regression in R Logistic regression in R is handled by the glm() function with the family parameter set to binomial. #Create a logistic regression model model_logistic &lt;- glm(y ~ x1 + x2, data = data, family = binomial) summary(model_logistic) For example, we want to analyse the factors that influence whether an arrest is made following a reported crime. # Load the dataset from csv file data_arrests &lt;- read.csv(&#39;data/data_arrests.csv&#39;) #Note that some of the variables need to be converted to factors data_arrests$Crime_Type = factor(data_arrests$Crime_Type) data_arrests$Time_of_Day = factor(data_arrests$Time_of_Day) #Create a logistic regression model model_logistic &lt;- glm(Arrest_Made ~ Crime_Type + Time_of_Day + High_Crime_Area + Police_On_Duty, data = data_arrests, family = binomial) summary(model_logistic) ## ## Call: ## glm(formula = Arrest_Made ~ Crime_Type + Time_of_Day + High_Crime_Area + ## Police_On_Duty, family = binomial, data = data_arrests) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.08619 0.70859 0.122 0.903 ## Crime_TypeBurglary 0.23511 0.51006 0.461 0.645 ## Crime_TypeRobbery 0.06021 0.48679 0.124 0.902 ## Crime_TypeTheft -0.27908 0.48041 -0.581 0.561 ## Time_of_DayNight 0.37125 0.35067 1.059 0.290 ## High_Crime_Area 0.32596 0.35093 0.929 0.353 ## Police_On_Duty 0.08227 0.05779 1.424 0.155 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 210.76 on 199 degrees of freedom ## Residual deviance: 205.49 on 193 degrees of freedom ## AIC: 219.49 ## ## Number of Fisher Scoring iterations: 4 8.6.2 Interpreting Logistic Regression Output Coefficients: In logistic regression, the coefficients represent the change in the log odds of the dependent variable for a one-unit increase in the predictor variable. To make the results more interpretable, you can convert the coefficients to odds ratios using the exp() function. # Convert coefficients to odds ratios exp(coef(model_logistic)) ## (Intercept) Crime_TypeBurglary Crime_TypeRobbery Crime_TypeTheft Time_of_DayNight High_Crime_Area Police_On_Duty ## 1.0900091 1.2650521 1.0620617 0.7564795 1.4495484 1.3853646 1.0857535 Odds Ratios: An odds ratio greater than 1 indicates that as the predictor increases, the odds of the outcome occurring (vs = 1) increase. An odds ratio less than 1 indicates a decrease in the odds. P-values: Similar to linear regression, p-values help determine whether the predictors are statistically significant. A p-value less than 0.05 typically suggests that the predictor has a significant effect on the outcome. Deviance: The null deviance represents a model with only the intercept, and the residual deviance represents the model with the predictors. A significant reduction in deviance suggests that the predictors improve the model. AIC (Akaike Information Criterion): A lower AIC value indicates a better-fitting model, accounting for model complexity. 8.7 Checking Model Assumptions for Logistic Regression Just like linear regression, logistic regression has assumptions that must be checked to ensure the model’s validity. 8.7.1 Assumption 1: Linearity of the Logit The relationship between continuous predictors and the logit of the outcome should be linear. You can check this by plotting the predictor against the logit of the predicted probabilities. How to Check: For each predictor, plot the logit of predicted probabilities against the predictor. # Predicted probabilities preds &lt;- predict(model_logistic, type = &quot;response&quot;) # Logit transformation logit &lt;- log(preds / (1 - preds)) # Plot logit vs predictor plot(data_arrests$Police_On_Duty, logit, xlab = &quot;Police on Duty&quot;, ylab = &quot;Logit of Predicted Probability&quot;) If the relationship is not linear, consider transforming the predictor or adding a nonlinear term. 8.7.2 Assumption 2: Independence of Observations Each observation should be independent. This assumption is more about the study design than something that can be directly tested through the data. How to Address: Ensure that the data collection process does not introduce dependencies between observations. 8.7.3 Assumption 3: Absence of Multicollinearity As with linear regression, multicollinearity can inflate the standard errors of the coefficients, making it hard to assess the effect of each predictor. How to Check: Use the Variance Inflation Factor (VIF) to detect multicollinearity. #Calculate Variance Inflation Factors vif(model_logistic) ## GVIF Df GVIF^(1/(2*Df)) ## Crime_Type 1.039076 3 1.006409 ## Time_of_Day 1.025253 1 1.012548 ## High_Crime_Area 1.015725 1 1.007832 ## Police_On_Duty 1.044253 1 1.021887 Dealing with Multicollinearity: If high VIF values are detected, consider removing or combining correlated predictors. 8.7.4 Assumption 4: Sufficient Sample Size Logistic regression requires a sufficiently large sample size, especially when the outcome is rare. A common rule of thumb is to have at least 10 events per predictor variable. How to Address: Ensure your sample size meets this criterion, particularly for rare outcomes. Exercise! The Police Dispatch Team prioritise some calls as “high-priority” to ensure a faster response and better allocation of resources. You have been tasked with determining what factors lead to a crime being categorised as “high-priority” or not. Using the following dataset data_response perform a logistic regression to identify which factors affect the high-priority crime decision. 8.8 Model Validation and Diagnostics After fitting the regression model and checking assumptions, it’s crucial to evaluate the model’s performance and address any potential issues such as overfitting. 8.8.1 Cross-Validation Cross-validation helps assess how the model generalises to an independent dataset. In k-fold cross-validation, the dataset is split into k subsets, and the model is trained k times, each time using a different subset as the validation set. How to Perform: The caret package in R provides an easy way to perform cross-validation. library(caret) ## Loading required package: ggplot2 ## Loading required package: lattice # Define cross-validation parameters train_control &lt;- trainControl(method = &quot;cv&quot;, number = 10) # Train the model using cross-validation model_cv &lt;- train(factor(Arrest_Made) ~ Crime_Type + Time_of_Day, data = data_arrests, method = &quot;glm&quot;, family = binomial, trControl = train_control) # View results print(model_cv) ## Generalized Linear Model ## ## 200 samples ## 2 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 179, 180, 180, 180, 180, 180, ... ## Resampling results: ## ## Accuracy Kappa ## 0.7801378 0 Cross-validation provides estimates of the model’s accuracy and can help prevent overfitting. 8.8.2 Dealing with Overfitting Overfitting occurs when the model is too complex, capturing noise rather than the underlying pattern. This can lead to poor performance on new data. Simplifying the Model: Reduce the number of predictors or use regularisation techniques like Lasso or Ridge regression. # Example of Lasso regression with glmnet package library(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.1-8 # Prepare data for glmnet x &lt;- as.matrix(data_arrests[, c(&quot;High_Crime_Area&quot;, &quot;Police_On_Duty&quot;)]) y &lt;- data_arrests$Arrest_Made # Fit Lasso model lasso_model &lt;- cv.glmnet(x, y, family = &quot;binomial&quot;, alpha = 1) # View best lambda value print(lasso_model$lambda.min) ## [1] 0.04493421 Regularisation methods add a penalty to the coefficients, shrinking less important ones towards zero, which helps to prevent overfitting. 8.9 Conclusion In this chapter, you have learned how to perform and interpret linear, multiple linear, and logistic regression in R, as well as how to validate your models and check their assumptions. Regression is a powerful tool, but its effectiveness depends on understanding and ensuring the assumptions are met. Key Points: Linear and Logistic Regression: Both types of regression help model relationships, with linear regression suited for continuous outcomes and logistic regression for binary outcomes. Model Assumptions: Checking assumptions is crucial for ensuring your model’s validity. Validation: Techniques like cross-validation help assess model performance and prevent overfitting. Comparison with SPSS: R offers more flexibility and control over the modeling process, though the core principles remain consistent. In the next chapter, we will explore another critical aspect of data analysis: Geographic Mapping and Spatial Analysis. You’ll learn how to work with geographic data, create maps, and perform spatial analysis in R, building on the skills you’ve developed in regression analysis. "],["geographic-mapping-and-spatial-analysis.html", "9 Geographic Mapping and Spatial Analysis 9.1 Introduction to Geographic Data in R 9.2 Geographic Coordinate Systems and Projections 9.3 Creating Basic Maps 9.4 Spatial Analysis 9.5 Conclusion", " 9 Geographic Mapping and Spatial Analysis 9.1 Introduction to Geographic Data in R Geographic mapping and spatial analysis allow you to visualise data in a geographical context, which can be crucial for identifying patterns, trends, and relationships. In this chapter, we’ll explore how to work with geographic data in R, create basic maps, and perform simple spatial analyses. SPSS lacks built-in support for geographic data formats, so users typically rely on external tools like GIS software for spatial analysis. R, however, provides comprehensive tools to manage and analyse spatial data within the same environment you use for statistical analysis. 9.1.1 Understanding Geographic Data Formats Geographic data comes in various formats. The most common formats you’ll encounter in R are: Shapefiles: A popular format for geographic data that consists of multiple files (.shp, .shx, .dbf) that store geometry and attribute information. GeoJSON: A JSON format that encodes a variety of geographic data structures. Other Formats: Other formats include KML, GPX, and spatial databases, all of which can be handled in R. 9.1.2 Importing and Handling Spatial Data with the sf Package The sf (simple features) package in R is designed for working with spatial data. It simplifies the process of importing, manipulating, and analysing geographic data. Installing and Loading the sf Package: #Install the sf packages install.packages(&quot;sf&quot;) #Load the sf package library(sf) Importing Shapefiles: # Import a shapefile spatial_data &lt;- st_read(&quot;path_to_your_shapefile.shp&quot;) Importing GeoJSON Files: # Import a GeoJSON file spatial_data &lt;- st_read(&quot;path_to_your_geojson.geojson&quot;) Exploring Spatial Data: You can explore Spatial Data by using the head() command and preview the base shapefile by using the plot() command. # View the structure of the spatial data head(spatial_data) # Plot the spatial data plot(st_geometry(spatial_data)) Exercise! Recreate these steps using the 2018 London Wards Shapefiles available at Statistical GIS Boundary Files for London. You should get the output shown below. ## Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE 9.2 Geographic Coordinate Systems and Projections 9.2.1 Introduction to Coordinate Systems When working with spatial data, it’s essential to understand the concept of geographic coordinate systems (GCS) and map projections. A geographic coordinate system uses latitude and longitude to define locations on the Earth’s surface. However, because the Earth is a three-dimensional shape (approximately an ellipsoid), representing it on a two-dimensional map requires a transformation, known as a map projection. Latitude and Longitude: These are the most common coordinates, often expressed in degrees. Latitude lines run parallel to the Equator, while longitude lines run from pole to pole. Projections: Map projections are methods of flattening the Earth’s surface onto a plane. Each projection distorts some aspect of the Earth’s surface (such as area, shape, distance, or direction), so choosing the correct projection is crucial for accurate spatial analysis. 9.2.2 Common Issues with Coordinate Systems in R When working with spatial data in R, you might encounter issues related to coordinate systems, particularly when combining datasets from different sources. Here are some common problems: Mismatched Coordinate Systems: If you try to combine or overlay spatial data from different sources that use different coordinate systems, the data might not align correctly. For example, one dataset might use a geographic coordinate system (e.g., WGS84), while another uses a projected coordinate system (e.g., UTM). Unprojected Data: Spatial data might be in a geographic coordinate system (unprojected) rather than a projected system, which can cause distortions, especially when measuring distances or areas. CRS (Coordinate Reference System) Errors: Sometimes, the coordinate reference system (CRS) of a spatial object might not be correctly defined, leading to misalignments or errors in analysis. 9.2.3 Handling Coordinate Systems in R The sf package in R provides tools to handle these issues effectively. Checking and Setting CRS: When you load spatial data, it’s essential to check and, if necessary, set the correct CRS. You can use the st_crs() function to check the CRS of an sf object and st_set_crs() to set it to a specific coordinate system. # Checking the CRS st_crs(spatial_data) # Setting the CRS to WGS84 (EPSG:4326) spatial_data &lt;- st_set_crs(spatial_data, 4326) Transforming CRS: If you need to align datasets with different CRSs, you can transform them using the st_transform() function. # Transforming to WGS84 (EPSG:4326) spatial_data_utm &lt;- st_transform(spatial_data, 4326) Dealing with Unprojected Data: If your data is in a geographic coordinate system (e.g., WGS84) and you need to perform distance calculations, you should transform it to a suitable projected coordinate system first to avoid distortions. # Transforming to a suitable projection for distance calculation spatial_data_projected &lt;- st_transform(spatial_data, 32633) 9.2.4 Practical Considerations Choosing the Right Projection: The choice of projection depends on the geographic extent of your data and the type of analysis you are performing. For local or regional analyses, UTM (Universal Transverse Mercator) is often a good choice because it minimises distortion within each zone. Reprojecting Data for Visualisation: When creating maps, you might need to reproject your data to a coordinate system that balances area, shape, and distance for your specific region. By understanding and correctly handling geographic coordinate systems, you can ensure that your spatial analyses in R are accurate and meaningful. This step is crucial in applications like crime mapping, where precision in geographic location can directly impact the insights drawn from the data. Exercise! Go to the EPSG website and explore the different coordinate options available for the United Kingdom. What is the EPSG code of the British National Grid projection? 9.3 Creating Basic Maps R provides powerful tools for creating maps. This section will show you how to plot your spatial data and customise maps using ggplot2 and the sf package. 9.3.1 Plotting Data on Maps Using ggplot2 and sf ggplot2, in combination with sf, makes it easy to create visually appealing maps. Creating a Simple Map: library(ggplot2) # Create a basic map ggplot(data = spatial_data) + geom_sf() + theme_minimal() Adding Data Points to a Map: # Assuming data_points is a data frame with longitude and latitude columns ggplot() + geom_sf(data = spatial_data) + geom_point(data = data_points, aes(x = longitude, y = latitude), color = &quot;red&quot;) + theme_minimal() Exercise! Use the London 2018 Ward Shapefile to create a map of London using ggplot2. Add a light blue point at (51.503618787766060, -0.09860017990274424) to represent the MOPAC offices. These coordinates were taken from Google Maps which uses a WGS84 projection which may differ from the projection used by the London Shapefile. Hint I: You will need to make a dataframe. Hint II: coord_sf() ensures that all layers use a common CRS. 9.3.2 Customising Maps Customising maps allows you to enhance their readability and aesthetic appeal. Customizing Colors and Labels: gglot2 makes it simple to change the map titles, labels, and axes. ggplot(data = spatial_data) + geom_sf(aes(fill = some_attribute)) + scale_fill_viridis_c() + labs(title = &quot;Your Map Title&quot;, fill = &quot;Legend Title&quot;) + theme_minimal() For example, we can colour each ward by its size in hectares. ggplot(data = spatial_data) + geom_sf(aes(fill = HECTARES)) + scale_fill_viridis_c() + labs(title = &quot;London Ward Map Coloured by Ward Size&quot;, fill = &#39;HECTARES&#39;) + theme_minimal() Adding Scales and Legends: Geographic maps in R are fully customisable. The following example demonstrates one way how the colouring and formatting can be adjusted. ggplot(data = spatial_data) + geom_sf(aes(fill = HECTARES)) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;blue&quot;) + labs(title = &quot;London Ward Map Coloured by Ward Size&quot;, fill = &quot;HECTARES&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) 9.4 Spatial Analysis Spatial analysis involves examining geographic patterns, relationships, and trends within your data. R allows you to perform various spatial operations that can enhance your geographic analysis. 9.4.1 Basic Spatial Operations Point-in-Polygon: Given a list of points you may want to identify which polygon they fall in. This can easily be done using the st_within command. # Check if points fall within polygons points_in_polygons &lt;- st_within(data_points, spatial_data) print(points_in_polygons) For example, let us randomly create some points across London and then identify which ward they occur in. # Set seed for reproducibility set.seed(123) # Number of points to generate n &lt;- 200 # Generate random latitude and longitude points within the bounding box of London # Approximate bounding box for London: # Latitude: 51.286760 to 51.691874 # Longitude: -0.510375 to 0.334015 latitudes &lt;- runif(n, min = 51.286760, max = 51.691874) longitudes &lt;- runif(n, min = -0.510375, max = 0.334015) # Create a dataframe london_coords &lt;- data.frame( Latitude = latitudes, Longitude = longitudes ) # View the first few rows of the dataframe head(london_coords) ## Latitude Longitude ## 1 51.40326 -0.308797130 ## 2 51.60611 0.302231262 ## 3 51.45244 -0.002587795 ## 4 51.64448 -0.075489049 ## 5 51.66776 -0.170446096 ## 6 51.30522 0.232896377 # Convert the dataframe to a spatial sf object london_coords_sf &lt;- st_as_sf(london_coords, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326) ## Check if points fall within polygons #Before we can do this both spatial objects need to be in the same CRS spatial_data_wsg &lt;- st_transform(spatial_data, 4326) #Check if points fall in polygon points_in_polygons &lt;- st_within(london_coords_sf, spatial_data_wsg) #Print the results print(points_in_polygons) ## Sparse geometry binary predicate list of length 200, where the predicate was `within&#39; ## first 10 elements: ## 1: 392 ## 2: (empty) ## 3: 269 ## 4: 325 ## 5: 311 ## 6: (empty) ## 7: 465 ## 8: (empty) ## 9: 87 ## 10: 84 #The results are not very informative so we can replace the number with the area Name # Extract the names of the areas where each point is located area_names &lt;- sapply(points_in_polygons, function(x) if(length(x) &gt; 0) spatial_data_wsg$NAME[x] else NA) # Add the area names to the original points dataframe london_coords_sf$NAME &lt;- area_names # View the updated dataframe with area names head(london_coords_sf) ## Simple feature collection with 6 features and 1 field ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -0.3087971 ymin: 51.30522 xmax: 0.3022313 ymax: 51.66776 ## Geodetic CRS: WGS 84 ## geometry NAME ## 1 POINT (-0.3087971 51.40326) Hampton Wick ## 2 POINT (0.3022313 51.60611) &lt;NA&gt; ## 3 POINT (-0.002587795 51.45244) Lewisham Central ## 4 POINT (-0.07548905 51.64448) Bush Hill Park ## 5 POINT (-0.1704461 51.66776) Cockfosters ## 6 POINT (0.2328964 51.30522) &lt;NA&gt; Distance Calculations: You can calculate the distance between two sets of points by using the st_distance command. # Calculate distances between points distances &lt;- st_distance(data_points, reference_points) # Print the distances print(distances) Note that the input should be a spatial object. Remember, you can convert a dataframe of points into a spatial object using the st_as_sf function. Exercise! The British Transport Police have provided you with a number of crimes reported at varying locations across London. You need to identify which Borough they have occurred in. Longitude Latitude Borough -0.19611111405556494 51.375258353438184 ? -0.30174186589110447 51.46320906930937 ? -0.19565513135661436 51.487172486590836 ? -0.13840606266909572 51.46153797665847 ? -0.09291579752832058 51.37534709116438 ? -0.13321537011710344 51.528459469086386 ? 9.4.2 Creating Choropleth Maps Choropleth maps are a powerful way to visualise data across geographic areas. Here’s how to create them in R: Basic Choropleth Map: ggplot(data = spatial_data) + geom_sf(aes(fill = your_variable)) + scale_fill_viridis_c() + labs(title = &quot;Choropleth Map&quot;, fill = &quot;Your Variable&quot;) + theme_minimal() The Ward Profiles Dataset provides a wealth of demographic and related data for each ward in Greater London. We can merge (remember back to chapter 3?) this dataset with our spatial data to provide informative spatial maps. #Load the ward profiles dataset ward_profiles &lt;- read.csv(&#39;data/ward_profiles.csv&#39;) #Join the demographic data to the Spatial dataset spatial_data &lt;- merge(spatial_data, ward_profiles, by.x = &quot;LAGSSCODE&quot;, by.y = &quot;New.code&quot;) #Create a Choropleth map of Employment Rates ggplot(data = spatial_data) + geom_sf(aes(fill = Employment.rate..16.64....2011)) + scale_fill_viridis_c() + labs(title = &quot;Choropleth Map of Employment Rates Across London&quot;, fill = &quot;Employment Rate&quot;) + theme_minimal() Exercise! Create a choropleth map of London visualising the Turnout.at.Mayoral.election...2012. Hint: Make sure to check the variable type 9.5 Conclusion In this chapter, we’ve explored the basics of geographic mapping and spatial analysis in R. You’ve learned how to import and manage geographic data, create basic maps, and perform spatial analysis tasks such as point-in-polygon operations and distance calculations. We’ve also seen how to create and customise choropleth maps. By mastering these skills, you’ll be able to extend your data analysis into the spatial domain, something that isn’t directly possible within SPSS. "],["exercise-answers.html", "Exercise Answers Chapter 2 Chapter 3 Chapter 5 Chapter 6 Chapter 7 Chapter 8 Chapter 9", " Exercise Answers Chapter 2 Exercise 2.1 Execute these operations in R and compare with results from SPSS’s Compute function. Can you do this in both the console and the script editor? addition &lt;- 5 + 3 subtraction &lt;- 10 - 4 multiplication &lt;- 7 * 2 division &lt;- 8 / 2 exponentiation &lt;- 2^3 Exercise 2.2 Test these logical operations in R and compare them to SPSS’s logical operators. # Logical operations equal &lt;- (5 == 5) # TRUE not_equal &lt;- (5 != 3) # TRUE greater_than &lt;- (5 &gt; 3) # TRUE and_operation &lt;- (5 &gt; 3 &amp; 4 &lt; 6) # TRUE or_operation &lt;- (5 &gt; 6 | 4 &lt; 6) # TRUE Exercise 2.3 Between January 2023 and December 2023 the number of Violent and Sexual Offences in Wandsworth were as follows: 568, 568, 603, 604, 685, 871, 697, 608, 657, 681, 630, and 720. (1) Create a vector representing the number of Violent and Sexual Offences in Wandsworth. (2) What was the mean number of crimes each month? (3) What was the total number of crimes across the year? (4) What is the length of the vector? #Create a vector representing all Violent &amp; Sexual Offences in Wandsworth in 2023 wandsworth_vso_2023 &lt;- c(568, 568, 603, 604, 685, 871, 697, 608, 657, 681, 630, 720) #Calculate the Monthly Average mean_wandsworth_vso_2023 &lt;- mean(wandsworth_vso_2023) paste(&quot;Mean Number of Monthly V&amp;S Offences in Wandsworth in 2023: &quot;, mean_wandsworth_vso_2023) ## [1] &quot;Mean Number of Monthly V&amp;S Offences in Wandsworth in 2023: 657.666666666667&quot; #Total Number of Crimes total_wandsworth_vso_2023 &lt;- sum(wandsworth_vso_2023) paste(&quot;Total Number of V&amp;S Offences in Wandsworth in 2023: &quot;, total_wandsworth_vso_2023) ## [1] &quot;Total Number of V&amp;S Offences in Wandsworth in 2023: 7892&quot; #Length of Vector paste(&quot;Length of Vector: &quot;, length(wandsworth_vso_2023)) ## [1] &quot;Length of Vector: 12&quot; Chapter 3 Exercise 3.1 Go to https://data.police.uk/data/ and download a CSV file of Metropolitan Police Service data for January 2024. Load the CSV file into a R dataframe called data_mps. How many observations and variables are there? #Load the data from CSV file mps_2024_01 &lt;- read.csv(&#39;data/2024-01-metropolitan-street.csv&#39;) #Return the size paste(&quot;Number of Observations: &quot;, dim(mps_2024_01)[1]) ## [1] &quot;Number of Observations: 91411&quot; paste(&quot;Number of Variables: &quot;, dim(mps_2024_01)[2]) ## [1] &quot;Number of Variables: 12&quot; ## Note that dim() finds the dimensions of the given variable and [1] or [1] indexes this to take the first or second part. ## Alternatively, you coudl find this in the environment pane. Exercise 3.2 Some of the commands above are composite commands encompassing multiple functions. Can you break them down to understand what each part of the function is doing? # Remove any columns with missing data data_mps_clean &lt;- data_mps[, colSums(is.na(data_mps)) == 0] # is.na(data_mps) identifies any NA values returning a Boolean, 1 = Yes, 0 = No # colSums(x) adds the values in each column # data_mps[, x] subsets the data to only select column x # Together, this function identifies the columns with no missing values, # Produces a cleaned subset by dropping an columns with missing values # Replace missing values with a specific value, such as 0. data_mps[is.na(data_mps)] &lt;- 0 # is.na(data_mps) identifies any NA values returning a Boolean, 1 = Yes, 0 = No # data_mps[x] &lt;- 0 sets the value x to zero # Together, this function identifies the missing data # Replaces the missing value with a 0. #Replace missing values with a column value (e.g. Mode, Mean, ...) data_mps$Latitude[is.na(data_mps$Latitude)] &lt;- mean(data_mps$Latitude, na.rm = TRUE) # mean(data_mps$Latitude, na.rm = TRUE) calculates the average latitude # na.rm = TRUE means it does this excluding the missing values # is.na(data_mps$Latitude) identifies any missing latitudes # Together, this function calculates the average latitude, excluding missing values # Assings any missing latitudes the average value. Exercise 3.3 In the examples above, in the second example we replaced all missing values with 0. We also replaced any missing latitudes with the average latitude across the dataset in the fourth example. Were these the best imputation choices? What method would you have chosen for dealing with outliers? It does not make analytical sense to replace all missing values in the dataset. We have a variety of data formats such as factors including the Crime Type, dates in the format of the Month, as well as character data for location. Each variable should be considered individually to perform the best possible strategy of dealing with NA values. It does not make sense to replace the missing latitudes with the average latitude as this could effect our analysis. Depending on the analysis to be performed and the number of missing values it would be better to drop observations with a missing latitude or set it to a value outside the geographic range of analysis. Exercise 3.4 The Context column is full of NA values. Can you subset the dataset to remove this column? Can you then filter the dataset to create a new dataframe called data_mps_drugs containing only crimes with the Crime Type recorded as ‘Drugs’. #Load dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:car&#39;: ## ## recode ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union #Using Separate Functions mps_2024_01_drop &lt;- select(mps_2024_01, -Context) mps_2024_01_drugs_1 &lt;- filter(mps_2024_01_drop, Crime.type == &#39;Drugs&#39;) #Using a dplyr pipeline mps_2024_01_drugs_2 &lt;- mps_2024_01 %&gt;% select(-Context) %&gt;% #Drop the &#39;Context&#39; column filter(Crime.type == &#39;Drugs&#39;) #Filter Crime Type Exercise 3.5 How would you merge two datasets if they didn’t have a common key? # Merge two datasets with different keys merged_data &lt;- merge(data1, data2, by.x = &#39;data1_key&#39;, by.y = &#39;data2.key&#39;) Exercise 3.6 Using the original data_mps dataframe can you recreate the data_mps_drugs dataframe using the dplyr pipeline operator with the relevant select and filter commands? #Using a dplyr pipeline to select and filter mps_2024_01_drugs &lt;- mps_2024_01 %&gt;% select(-Context) %&gt;% #Drop the &#39;Context&#39; column filter(Crime.type == &#39;Drugs&#39;) #Filter Crime Type Exercise 3.7 Using the unfiltered data_mps dataset. Convert the Crime Type columns into a factor. #Convert the Crime Type column into a factor. mps_2024_01$Crime.type &lt;- factor(mps_2024_01$Crime.type) Exercise 3.8 Convert the Last Outcome Category to a categorical variable and view a cross-tabulation of the outcomes with the crime types. Which Crime Type has the highest number of Complete Investigations Where No Suspect Was Identified? #Convert the Last Outcome column into a factor. mps_2024_01$Last.outcome.category &lt;- factor(mps_2024_01$Last.outcome.category) #Create cross-tabulation table(mps_2024_01$Crime.type , mps_2024_01$Last.outcome.category) ## ## Awaiting court outcome Investigation complete; no suspect identified Local resolution Offender given a caution ## Anti-social behaviour 16514 0 0 0 0 ## Bicycle theft 0 1 672 0 0 ## Burglary 0 61 2257 1 1 ## Criminal damage and arson 0 78 2721 18 25 ## Drugs 0 228 318 627 37 ## Other crime 0 36 245 28 3 ## Other theft 0 14 6252 5 2 ## Possession of weapons 0 78 52 5 5 ## Public order 0 95 1460 7 3 ## Robbery 0 39 778 1 0 ## Shoplifting 0 122 3375 81 13 ## Theft from the person 0 13 4296 2 1 ## Vehicle crime 0 20 6487 0 0 ## Violence and sexual offences 0 454 6199 44 68 ## ## Offender given a drugs possession warning Offender given penalty notice Status update unavailable ## Anti-social behaviour 0 0 0 ## Bicycle theft 0 0 296 ## Burglary 0 0 2762 ## Criminal damage and arson 0 4 2028 ## Drugs 7 169 1327 ## Other crime 0 0 519 ## Other theft 0 0 4024 ## Possession of weapons 0 0 269 ## Public order 0 2 2592 ## Robbery 0 0 2128 ## Shoplifting 0 19 1443 ## Theft from the person 0 0 2603 ## Vehicle crime 0 0 2463 ## Violence and sexual offences 0 1 14913 The VEHICLE CRIME, crime type has the highest number of Complete Investigations Where No Suspect Was Identified. Chapter 5 Exercise 5.1 Download the police_activity_data.csv file and load it into a R data frame. Produce a basic summary of the Response Time variable. What does the difference between the Mean and Median measure tell you about the skewness of the data? #Load data police_activity_data &lt;- read.csv(&#39;data/police_activity_data.csv&#39;) #Summary of Response Time Variable summary(police_activity_data$ResponseTime) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 9.00 13.00 12.42 16.00 20.00 The Mean (12.42) is SMALLER than the Median (13.00). When the mean of a dataset is smaller than the median, it generally indicates that the data distribution is left-skewed (or negatively skewed). In a left-skewed distribution, the tail on the left side of the distribution is longer than the tail on the right side. This means there are a few low values that pull the mean down, while the majority of the data are higher, which positions the median closer to the upper range of the data. Excercise 5.2 Create a Bar Chart of the Borough variable in the police_activity_data dataset. Which borough has the greatest number of crimes? #Load the ggplot2 library library(ggplot2) # Bar chart for a categorical variable ggplot(police_activity_data, aes(x = Borough)) + geom_bar() + labs(title = &quot;Bar Chart Displaying the Number of Incidents in Each Borough in August 2024&quot;, x = &quot;Borough&quot;, y = &quot;Number of Incidents&quot;) &gt; The NORTH borough had the most incidents in the period in question. Exercise 5.3 Create a Histogram of the ResponseTime variable in the police_activity_data dataset setting the bin size to 4. How is the response time distributed? ggplot(police_activity_data, aes(x = ResponseTime)) + geom_histogram(binwidth = 4, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = &quot;Histogram of Response Time&quot;, x = &quot;Response Time (Minutes)&quot;, y = &quot;Frequency&quot;) The histogram shows the distribution of response times, with most values concentrated between 8 and 16 minutes. The distribution appears roughly normal, with no strong skewness, indicating a fairly even spread of response times across this range. The tails on both sides are shorter, suggesting fewer instances of very short (less than 5 minutes) or very long (more than 20 minutes) response times. Exercise 5.4 Create a Boxplot of the ResponseTime variable for each of the Borough in the police_activity_data dataset. Which Borough has the lowest Median response time? Which Borough has the smallest range of response times? ggplot(police_activity_data, aes(x = Borough, y = ResponseTime)) + geom_boxplot() + labs(title = &quot;Boxplot of Response Time by Borough&quot;, x = &quot;Borough&quot;, y = &quot;Response Time&quot;) &gt; The EAST borough has the lowest MEDIAN RESPONSE TIME. &gt; The WEST borough had the smallest RANGE OF RESPONSE TIMES. Exercise 5.6 Using your Boxplot Diagram of the ResponseTime variable for each of the Borough in the police_activity_data dataset. Add some colour! ggplot(police_activity_data, aes(x = Borough, y = ResponseTime, fill = Borough)) + geom_boxplot() + labs(title = &quot;Boxplot of Response Time by Borough&quot;, x = &quot;Borough&quot;, y = &quot;Response Time&quot;) + scale_fill_brewer(palette = &quot;Pastel1&quot;) Exercise 5.7 Using the police_activity_data dataset, calculate the mean and standard deviation of the ResponseTime based on the IncidentType. Which Incident Type had the greatest mean response time? # Load the dplyr library library(dplyr) # Summarise data: mean and standard deviation by group police_activity_data_summary &lt;- police_activity_data %&gt;% group_by(IncidentType) %&gt;% summarize( mean_value = mean(ResponseTime, na.rm = TRUE), sd_value = sd(ResponseTime, na.rm = TRUE) ) #Print results print(police_activity_data_summary) ## # A tibble: 5 × 3 ## IncidentType mean_value sd_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Assault 11.8 4.14 ## 2 Burglary 12.2 4.30 ## 3 Public Disturbance 11.3 4.12 ## 4 Robbery 13.4 4.07 ## 5 Traffic Stop 13.4 4.58 ROBBERY had the GREATEST MEAN RESPONSE TIME. Exercise 5.8 Building on the previous exercise, create a plot of the mean ResponseTime based on the IncidentType. The factors along the x-axis should be sorted alphabetically. Can you try sorting these in ascending order of their mean? ggplot(police_activity_data_summary, aes(x = reorder(IncidentType, mean_value), y = mean_value)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Mean Response Time by Incident Type&quot;, x = &quot;Incident Type&quot;, y = &quot;Mean Response Time&quot;) Excerise 5.9 Use the facet wrap functionality to create a series of histograms representing the IncidentSeverity across the four different boroughs using a binwidth of 3. How does the response time vary across the four different boroughs? ggplot(police_activity_data, aes(x = IncidentSeverity)) + geom_histogram(binwidth = 3) + facet_wrap(~Borough) + labs(title = &quot;Histogram of Incident Severity Faceted by Borough&quot;) The distribution of incident severity is fairly consistent across all boroughs, with most incidents falling within the mid-range of severity on the severity scale. Exercise 5.10 Using dplyr produce a count of the number of crimes that occurred on each day. Use this information to create a scatterplot with a regression line. What do you notice about the trend? summary_data &lt;- police_activity_data %&gt;% group_by(Date) %&gt;% # Group by the &#39;Date&#39; column summarize(count = n()) # Produce a Count ggplot(summary_data , aes(x = as.Date(Date), y = count)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Scatter Plot of Number of Crimes Across August 2024 with Regression Line&quot;, x = &quot;Date&quot;, y = &quot;Number of Crimes&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Chapter 6 You have a dataset from a national survey on perceptions of crime. This dataset provides a comprehensive analysis of crime and policing by examining crime types, victim demographics, police responses, community engagement, and socioeconomic factors to understand the respondents crime experiences and perceptions. Download the Crime Survey Data (SPSS Format) and perform a descriptive analysis of the data. The steps you need to follow are listed below to help you. #Load haven library library(haven) #load Survey library library(survey) ## Loading required package: grid ## Loading required package: survival ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## cluster ## ## Attaching package: &#39;survey&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## dotchart #Load the dataset survey_data &lt;- read_sav(&quot;data/crime_survey_data.sav&quot;) #View loaded data head(survey_data) ## # A tibble: 6 × 21 ## id region crime_type satisfaction programs gender incident_severity response_time victim_age victim_gender victim_ethnicity police_action community_engagement ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 East Assault 5 1 Male Serious 8 55+ Other Asian Warning 3 ## 2 2 East Vandalism 2 6 Female Minor 91 18-24 Female Asian Arrest 2 ## 3 3 East Vandalism 3 3 Male Moderate 18 18-24 Other Hispanic Investigation 1 ## 4 4 South Assault 1 5 Male Serious 120 18-24 Other Asian Investigation 1 ## 5 5 East Theft 5 1 Female Minor 15 18-24 Female Black Arrest 1 ## 6 6 South Theft 3 6 Female Minor 55 Under 18 Female Black Warning 4 ## # ℹ 8 more variables: perceived_crime_rate &lt;chr&gt;, education &lt;chr&gt;, employment_status &lt;chr&gt;, household_income &lt;chr&gt;, living_situation &lt;chr&gt;, previous_victimization &lt;chr&gt;, ## # neighborhood_safety &lt;dbl&gt;, weight &lt;dbl&gt; #Create a survey design object survey_design &lt;- svydesign( id = ~id, weights = ~weight, data = survey_data ) #Create a frequency distribution of the crime_type svytable(~crime_type, design = survey_design) ## crime_type ## Assault Burglary Theft Vandalism ## 146.0768 142.8698 161.8578 172.4434 # and gender variable. svytable(~gender, design = survey_design) ## gender ## Female Male ## 284.1523 339.0955 # Load the janitor package library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test #Cross-tabulate crime_type by region and gender (separately and together) to see the distribution across different regions and genders. # Cross-tabulation with janitor: Crime Type &amp; Region crime_region_tab &lt;- svytable(~crime_type + region, survey_design) # Print results print(crime_region_tab) ## region ## crime_type East North South West ## Assault 38.00701 37.61599 32.31634 38.13746 ## Burglary 26.63030 45.57457 47.72543 22.93950 ## Theft 34.02101 50.79713 45.24576 31.79386 ## Vandalism 51.57803 46.30077 42.95608 31.60851 # Cross-tabulation with janitor: Crime Type &amp; Gender crime_gender_tab &lt;- svytable(~crime_type + gender, survey_design) # Print results print(crime_region_tab) ## region ## crime_type East North South West ## Assault 38.00701 37.61599 32.31634 38.13746 ## Burglary 26.63030 45.57457 47.72543 22.93950 ## Theft 34.02101 50.79713 45.24576 31.79386 ## Vandalism 51.57803 46.30077 42.95608 31.60851 ## Cross-tabulation with survey: Crime Type &amp; Region + Gender crime_region_gender_tab &lt;- svytable(~crime_type + region + gender, survey_design) # Print results print(crime_region_gender_tab) ## , , gender = Female ## ## region ## crime_type East North South West ## Assault 16.89515 11.89451 17.06074 17.06962 ## Burglary 12.62156 24.57370 18.88147 11.25734 ## Theft 13.91476 24.26910 24.08906 15.52493 ## Vandalism 27.34231 23.79401 10.02678 14.93727 ## ## , , gender = Male ## ## region ## crime_type East North South West ## Assault 21.11187 25.72148 15.25560 21.06784 ## Burglary 14.00874 21.00087 28.84396 11.68215 ## Theft 20.10625 26.52803 21.15670 16.26893 ## Vandalism 24.23571 22.50676 32.92930 16.67125 #Create a new variable to categorise satisfaction into high and low satisfaction (above 3 = High, 3 or below = Low). survey_data &lt;- survey_data %&gt;% mutate(satisfaction_rank = ifelse(satisfaction &gt; 3, &quot;High&quot;, &quot;Low&quot;)) #Output check head(survey_data) ## # A tibble: 6 × 22 ## id region crime_type satisfaction programs gender incident_severity response_time victim_age victim_gender victim_ethnicity police_action community_engagement ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 East Assault 5 1 Male Serious 8 55+ Other Asian Warning 3 ## 2 2 East Vandalism 2 6 Female Minor 91 18-24 Female Asian Arrest 2 ## 3 3 East Vandalism 3 3 Male Moderate 18 18-24 Other Hispanic Investigation 1 ## 4 4 South Assault 1 5 Male Serious 120 18-24 Other Asian Investigation 1 ## 5 5 East Theft 5 1 Female Minor 15 18-24 Female Black Arrest 1 ## 6 6 South Theft 3 6 Female Minor 55 Under 18 Female Black Warning 4 ## # ℹ 9 more variables: perceived_crime_rate &lt;chr&gt;, education &lt;chr&gt;, employment_status &lt;chr&gt;, household_income &lt;chr&gt;, living_situation &lt;chr&gt;, previous_victimization &lt;chr&gt;, ## # neighborhood_safety &lt;dbl&gt;, weight &lt;dbl&gt;, satisfaction_rank &lt;chr&gt; #Update the survey design with the new variable. survey_design_updated &lt;- update(survey_design, satisfaction_rank = survey_data$satisfaction_rank) # Mean number of community programs by satisfaction level mean_programs_satisfaction &lt;- svyby(~programs, ~satisfaction_rank, survey_design_updated, svymean) # Print results print(mean_programs_satisfaction) ## satisfaction_rank programs se ## High High 5.536490 0.2209763 ## Low Low 5.665286 0.1801161 # Mean number of community programs by gender mean_programs_gender &lt;- svyby(~programs, ~gender, survey_design_updated, svymean) # Print results print(mean_programs_gender) ## gender programs se ## Female Female 5.423680 0.2140237 ## Male Male 5.774012 0.1823737 Chapter 7 Exercise 7.1 The Metropolitan Police Service have implemented a new strategy to target Violence Against Women and Girls in 6 wards. The number of incidents in March across these 6 wards was 100, 102, 104, 106, and 108. After the implementation of the new strategy the number of incidents in April across these 6 wards was 110, 111, 115, 117, 120 respectively. Has the new strategy had an impact on the number of incidents? #Input the pre-treatment values pre_treatment &lt;- c(100, 102, 104, 106, 108) #Input the post-treatment values post_treatment &lt;- c(110, 111, 115, 117, 120) t.test(pre_treatment, post_treatment, paired = TRUE) ## ## Paired t-test ## ## data: pre_treatment and post_treatment ## t = -20.788, df = 4, p-value = 3.164e-05 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -12.015715 -9.184285 ## sample estimates: ## mean difference ## -10.6 The p-value is smaller than 0.05 indicating statistical significance. A significant result indicates that the mean difference between the pre-treatment and post-treatment measures is unlikely to have occurred by chance. In other words, the treatment (or intervention) has had a measurable effect on the outcome variable. Exercise 7.2 You want to identify if the number of Stop and Search performed in the month of September across 6 Boroughs where the Stop and Search occurred is Uniformly distributed. Use the following data to identify if the Number of Stop and Search performed is uniform across the 6 Boroughs. Richmond: 36 // Kingston: 25 // Merton: 28 // Sutton: 34 // Croydon: 42 // Wandsworth: 32 #Observed frequencies of Stop &amp; Search observed_frequencies &lt;- c(35, 25, 28, 34, 42, 32) # Expected frequencies under the null hypothesis (uniform distribution) expected_frequencies &lt;- rep(sum(observed_frequencies) / length(observed_frequencies), length(observed_frequencies)) # Perform the Chi-Square Goodness-of-Fit Test chisq_test_goodness_of_fit &lt;- chisq.test(observed_frequencies, p = rep(1/length(observed_frequencies), length(observed_frequencies))) # Print the results print(chisq_test_goodness_of_fit) ## ## Chi-squared test for given probabilities ## ## data: observed_frequencies ## X-squared = 5.3673, df = 5, p-value = 0.3727 The p-value represents the probability of observing the data, or something more extreme, assuming that the null hypothesis is true. A p-value greater than 0.05 indicates that there is not enough evidence to reject the null hypothesis at the 5% significance level. In other words, the observed data does not significantly deviate from what would be expected under the null hypothesis. Exercise 7.3 Local residents in 3 boroughs were asked to rate their confidence in the Metropolitan Police Service rating their scores from 0 to 10. The average score across a 3 month period is as follows. Kingston : 7.0, 7.5, 8.2 Richmond : 8.3, 6.4, 7.9 Sutton : 6.4, 4.5, 5.8 Does the mean confidence level across the 3 month preiod differ between the three boroughs? # One-way ANOVA # Comparing scores across three different groups kingston &lt;- c(7.0, 7.5, 8.2) richmond &lt;- c(8.3, 6.4, 7.9) sutton &lt;- c(6.4, 4.5, 5.8) #Convert to dataframe data &lt;- data.frame( score = c(kingston, richmond, sutton), group = factor(rep(1:3, each = 3)) ) #Run anova anova_result &lt;- aov(score ~ group, data = data) #Print result summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 7.869 3.934 5.11 0.0506 . ## Residuals 6 4.620 0.770 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 F-value: This is the ratio of the Mean Square for the group to the Mean Square for the residuals. It tests whether the variability between groups is significantly greater than the variability within groups. The F-value of 5.11 suggests that there is some evidence that the means of the groups differ. p-value: This p-value tells us the probability of observing an F-value as extreme as 5.11, assuming the null hypothesis is true (i.e., assuming there is no difference between group means). A p-value of 0.0506 is just above the commonly used significance level of 0.05. The null hypothesis for ANOVA is that all group means are equal (i.e., there is no significant difference between the means of the groups). The result suggests that there is some evidence to reject the null hypothesis, but it is not strong enough to meet the conventional significance level of 0.05. Exercise 7.4 You have data on the number of community outreach programs conducted in 6 boroughs as well as the associated crime rates. You want to determine if there’s a linear relationship between these two variables. Use the data below to perform a correlation analysis. #Create dataframe data_outreach &lt;- data.frame( num_crimes = c(145, 154, 218, 255, 234, 189), community_prog = c(4, 5, 8, 19, 17, 15) ) # View the dataset print(data_outreach) ## num_crimes community_prog ## 1 145 4 ## 2 154 5 ## 3 218 8 ## 4 255 19 ## 5 234 17 ## 6 189 15 #Run pearson correlation cor(data_outreach$num_crimes, data_outreach$community_prog) ## [1] 0.8454724 # Creating and visualizing a correlation matrix library(corrplot) # Compute the correlation matrix cor_matrix &lt;- cor(data_outreach) # Visualize the correlation matrix corrplot(cor_matrix, method = &quot;circle&quot;) There is a strong positive correlation between the number of crimes and the number of community outreach programmes. Chapter 8 Exercise 8.1 Residential burglaries are on the up and you want to assess which factors are contributing to the increase. Using the following dataset data_burglaries perform a multiple linear regression The code has been produced here but the interpretation and summary are left to the reader. The code displayed below performs a stepwise linear regression, going outside the bounds of the original treaching. The assumptions are then tested and a box-cox transformation applied before recompleting the regression modelling. #Load dplyr library(dplyr) # Load the dataset from csv file data_burglaries &lt;- read.csv(&#39;data/data_burglaries.csv&#39;) # Data Overview - Check variable type head(data_burglaries) ## burglaries police_patrols avg_income neighbourhood_watch vacancy_rate full_time_employment avg_household_size crime_rate ## 1 124 8 48645.21 0 0.04594669 0.5158787 3.440589 2.315514 ## 2 197 9 53218.08 1 0.05548609 0.5732118 2.028175 5.005712 ## 3 45 14 38922.08 0 0.01165823 0.8515365 3.699788 4.557348 ## 4 58 10 41384.17 0 0.10697253 0.5327804 2.971972 4.977908 ## 5 18 10 30244.76 0 0.08263841 0.8109035 3.262457 3.849165 ## 6 106 15 47256.12 1 0.02037981 0.9686637 3.311017 3.626369 # Check for Missing Values any(is.na(data_burglaries)) ## [1] FALSE # Convert Neighbourhood Watch to factor data_burglaries$neighbourhood_watch &lt;- factor(data_burglaries$neighbourhood_watch) ## Perform a Stepwise Linear Regression # A stepwise linear regression uses a full model and a null model and # adds or remove variables until it has found the best model. # Fit a full model full_model &lt;- lm(burglaries ~ ., data = data_burglaries) # Fit a null model null_model &lt;- lm(burglaries ~ 1, data = data_burglaries) # Perform stepwise regression using both forward and backward selection stepwise_model &lt;- step(null_model, scope = list(lower = null_model, upper = full_model), direction = &quot;both&quot;) ## Start: AIC=2708.56 ## burglaries ~ 1 ## ## Df Sum of Sq RSS AIC ## + avg_income 1 72190727 78558628 2580.2 ## &lt;none&gt; 150749355 2708.6 ## + crime_rate 1 706661 150042694 2709.6 ## + police_patrols 1 649189 150100166 2709.7 ## + avg_household_size 1 469544 150279811 2709.9 ## + full_time_employment 1 97074 150652281 2710.4 ## + vacancy_rate 1 75824 150673531 2710.5 ## + neighbourhood_watch 1 36248 150713107 2710.5 ## ## Step: AIC=2580.21 ## burglaries ~ avg_income ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 78558628 2580.2 ## + full_time_employment 1 282352 78276276 2581.5 ## + avg_household_size 1 127046 78431582 2581.9 ## + neighbourhood_watch 1 70409 78488220 2582.0 ## + vacancy_rate 1 18141 78540487 2582.2 ## + police_patrols 1 9281 78549347 2582.2 ## + crime_rate 1 279 78558349 2582.2 ## - avg_income 1 72190727 150749355 2708.6 # View the results summary(stepwise_model) ## ## Call: ## lm(formula = burglaries ~ avg_income, data = data_burglaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -437.7 -354.9 -199.1 99.7 4640.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.593e+03 1.567e+02 -10.17 &lt;2e-16 *** ## avg_income 4.020e-02 2.980e-03 13.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 629.9 on 198 degrees of freedom ## Multiple R-squared: 0.4789, Adjusted R-squared: 0.4762 ## F-statistic: 182 on 1 and 198 DF, p-value: &lt; 2.2e-16 ### Check Assumptions ## Linearity #Plot residuals vs fitted values plot(stepwise_model$fitted.values, resid(stepwise_model)) #Add horizontal line at y = 0 abline(h = 0, col = &quot;red&quot;) ## Normality of Residuals #Create a Q-Q plot qqnorm(resid(stepwise_model)) #Add Q-Q line qqline(resid(stepwise_model), col = &quot;red&quot;) ## Heteroscedasticity #Plot Scale-Location plot plot(stepwise_model, which = 3) ## Independence of Errors #Load the car library library(car) #Run the DurbinWatson test durbinWatsonTest(stepwise_model) ## lag Autocorrelation D-W Statistic p-value ## 1 0.03975713 1.919275 0.512 ## Alternative hypothesis: rho != 0 ## Variance Inflation Factors #Calculate the Variance Inflation Factors #vif(stepwise_model) ## Box-Cox Transformation #Load MASS Library library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select #Perform the Box-Cox transformation bc &lt;- boxcox(stepwise_model, lambda = seq(-2, 2, by = 0.1)) #Find the optimal lambda optimal_lambda &lt;- bc$x[which.max(bc$y)] #Print optimal lambda print(paste(&quot;Optimal lambda:&quot;, optimal_lambda)) ## [1] &quot;Optimal lambda: 0.0202020202020203&quot; #Apply the Box-Cox transformation data_burglaries$burglaries_transformed &lt;- ifelse(optimal_lambda == 0, log(data_burglaries$burglaries), (data_burglaries$burglaries^optimal_lambda - 1) / optimal_lambda) # Fit a full model full_model_trans &lt;- lm(burglaries_transformed ~ ., data = data_burglaries) # Fit a null model null_model_trans &lt;- lm(burglaries_transformed ~ 1, data = data_burglaries) # Perform stepwise regression using both forward and backward selection transformed_model &lt;- step(null_model_trans, scope = list(lower = null_model_trans, upper = full_model_trans), direction = &quot;both&quot;) ## Start: AIC=-13861.95 ## burglaries_transformed ~ 1 ## Warning: attempting model selection on an essentially perfect fit is nonsense ## Df Sum of Sq RSS AIC ## + full_time_employment 1 2.1627e-30 1.5482e-28 -13863 ## &lt;none&gt; 1.5698e-28 -13862 ## + crime_rate 1 1.3196e-30 1.5566e-28 -13862 ## + neighbourhood_watch 1 8.3766e-31 1.5615e-28 -13861 ## + avg_household_size 1 3.8680e-31 1.5660e-28 -13860 ## + vacancy_rate 1 2.9754e-31 1.5669e-28 -13860 ## + police_patrols 1 2.7859e-31 1.5670e-28 -13860 ## + burglaries 1 1.0033e-31 1.5688e-28 -13860 ## + avg_income 1 1.1260e-32 1.5697e-28 -13860 ## ## Step: AIC=-13862.72 ## burglaries_transformed ~ full_time_employment ## Warning: attempting model selection on an essentially perfect fit is nonsense ## Warning: attempting model selection on an essentially perfect fit is nonsense ## Df Sum of Sq RSS AIC ## + crime_rate 1 1.6739e-30 1.5315e-28 -13863 ## &lt;none&gt; 1.5482e-28 -13863 ## + neighbourhood_watch 1 1.3027e-30 1.5352e-28 -13862 ## - full_time_employment 1 2.1627e-30 1.5698e-28 -13862 ## + vacancy_rate 1 3.6578e-31 1.5445e-28 -13861 ## + avg_household_size 1 3.5445e-31 1.5447e-28 -13861 ## + police_patrols 1 3.5320e-31 1.5447e-28 -13861 ## + burglaries 1 7.8130e-32 1.5474e-28 -13861 ## + avg_income 1 2.0780e-32 1.5480e-28 -13861 ## ## Step: AIC=-13862.89 ## burglaries_transformed ~ full_time_employment + crime_rate ## Warning: attempting model selection on an essentially perfect fit is nonsense ## Warning: attempting model selection on an essentially perfect fit is nonsense ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1.5315e-28 -13863 ## - crime_rate 1 1.6739e-30 1.5482e-28 -13863 ## + neighbourhood_watch 1 1.2242e-30 1.5192e-28 -13862 ## - full_time_employment 1 2.5170e-30 1.5566e-28 -13862 ## + vacancy_rate 1 3.9426e-31 1.5275e-28 -13861 ## + police_patrols 1 2.8213e-31 1.5286e-28 -13861 ## + avg_household_size 1 2.5114e-31 1.5290e-28 -13861 ## + burglaries 1 3.5290e-32 1.5311e-28 -13861 ## + avg_income 1 2.6000e-34 1.5315e-28 -13861 # View the summary of the transformed model summary(transformed_model) ## Warning in summary.lm(transformed_model): essentially perfect fit: summary may be unreliable ## ## Call: ## lm(formula = burglaries_transformed ~ full_time_employment + ## crime_rate, data = data_burglaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.219e-14 -2.920e-17 6.130e-17 1.650e-16 4.187e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.063e+00 3.859e-16 1.312e+16 &lt;2e-16 *** ## full_time_employment 8.097e-16 4.500e-16 1.799e+00 0.0735 . ## crime_rate 4.414e-17 3.008e-17 1.467e+00 0.1439 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.817e-16 on 197 degrees of freedom ## Multiple R-squared: 0.5024, Adjusted R-squared: 0.4973 ## F-statistic: 99.45 on 2 and 197 DF, p-value: &lt; 2.2e-16 Exercise 8.2 The Police Dispatch Team prioritise some calls as “high-priority” to ensure a faster response and better allocation of resources. You have been tasked with determining what factors lead to a crime being categorised as “high-priority” or not. Using the following dataset data_response perform a logistic regression to identify which factors affect the high-priority crime decision. # Load the dataset from csv file data_response &lt;- read.csv(&#39;data/data_response.csv&#39;) # Data Overview - Check variable type head(data_response) ## High_Priority Crime_Severity Weapon_Involved Time_of_Day High_Crime_Area ## 1 1 2 1 Day 1 ## 2 0 3 0 Day 1 ## 3 1 8 1 Night 1 ## 4 1 9 0 Night 0 ## 5 1 8 1 Night 0 ## 6 1 4 0 Day 1 #Convert factor variables data_response$Weapon_Involved &lt;- factor(data_response$Weapon_Involved) data_response$Time_of_Day &lt;- factor(data_response$Time_of_Day) data_response$High_Crime_Area &lt;- factor(data_response$High_Crime_Area) #Create a logistic regression model model_logistic &lt;- glm(High_Priority ~ ., data = data_response, family = binomial) #Produce model summary summary(model_logistic) ## ## Call: ## glm(formula = High_Priority ~ ., family = binomial, data = data_response) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.83922 0.59431 -1.412 0.157921 ## Crime_Severity 0.22292 0.08811 2.530 0.011407 * ## Weapon_Involved1 1.87159 0.53179 3.519 0.000433 *** ## Time_of_DayNight 1.03824 0.46179 2.248 0.024556 * ## High_Crime_Area1 0.69768 0.44035 1.584 0.113107 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 169.08 on 199 degrees of freedom ## Residual deviance: 138.41 on 195 degrees of freedom ## AIC: 148.41 ## ## Number of Fisher Scoring iterations: 6 # Convert coefficients to odds ratios exp(coef(model_logistic)) ## (Intercept) Crime_Severity Weapon_Involved1 Time_of_DayNight High_Crime_Area1 ## 0.4320481 1.2497259 6.4986325 2.8242367 2.0090799 ## Check Model Assumptions # Linearity of Logit # Predicted probabilities preds &lt;- predict(model_logistic, type = &quot;response&quot;) # Logit transformation logit &lt;- log(preds / (1 - preds)) # Plot logit vs predictor plot(data_arrests$Police_On_Duty, logit, xlab = &quot;Police on Duty&quot;, ylab = &quot;Logit of Predicted Probability&quot;) # Multicolinearity #Calculate Variance Inflation Factors vif(model_logistic) ## Crime_Severity Weapon_Involved Time_of_Day High_Crime_Area ## 1.014402 1.017584 1.026008 1.016278 The model shows that incidents where a weapon is involved, the potential crime severity, and it being night time lead to an increased chance of a call being classed as high priority. The model assumptions have been displayed using code and it is left as a task of the reader to assess them. Chapter 9 Exercise 9.1 Recreate these steps using the 2018 London Wards Shapefiles available at Statistical GIS Boundary Files for London. You should get the output shown below. #Load the sf package library(sf) # Import a shapefile spatial_data &lt;- st_read(&quot;data/london-wards-2018/London_Ward.shp&quot;) ## Reading layer `London_Ward&#39; from data source `D:\\Daniel\\My Documents\\MOPAC\\Introduction to R\\data\\london-wards-2018\\London_Ward.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 657 features and 6 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## Projected CRS: OSGB36 / British National Grid # View the structure of the spatial data head(spatial_data) ## Simple feature collection with 6 features and 6 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 516362.6 ymin: 159907.4 xmax: 522194.7 ymax: 172367 ## Projected CRS: OSGB36 / British National Grid ## NAME GSS_CODE DISTRICT LAGSSCODE HECTARES NONLD_AREA geometry ## 1 Chessington South E05000405 Kingston upon Thames E09000021 755.173 0 POLYGON ((516401.6 160201.8... ## 2 Tolworth and Hook Rise E05000414 Kingston upon Thames E09000021 259.464 0 POLYGON ((519553 164295.6, ... ## 3 Berrylands E05000401 Kingston upon Thames E09000021 145.390 0 POLYGON ((518107.5 167303.4... ## 4 Alexandra E05000400 Kingston upon Thames E09000021 268.506 0 POLYGON ((520336.7 165105.5... ## 5 Beverley E05000402 Kingston upon Thames E09000021 187.821 0 POLYGON ((521201.2 169275.5... ## 6 Coombe Hill E05000406 Kingston upon Thames E09000021 442.170 0 POLYGON ((521296.5 172258.3... # Plot the spatial data plot(st_geometry(spatial_data)) Exercise 9.2 Go to the EPSG website and explore the different coordinate options available for the United Kingdom. What is the EPSG code of the British National Grid projection? EPSG:27700 - OSGB36 / British National Grid – United Kingdom Ordnance Survey Exercise 9.3 Use the London 2018 Ward Shapefile to create a map of London using ggplot2. Add a light blue point at (51.503618787766060, -0.09860017990274424) to represent the MOPAC offices. These coordinates were taken from Google Maps which uses a WGS84 projection which may differ from the projection used by the London Shapefile. data_points &lt;- data.frame(longitude = -0.09860017990274424, latitude = 51.503618787766060) ggplot() + geom_sf(data = spatial_data) + geom_point(data = data_points, aes(x = longitude, y = latitude), color = &quot;lightblue&quot;) + coord_sf(crs = st_crs(4326)) + ggtitle(&quot;Geographic Ward Level Plot of London&quot;, subtitle = &quot;With a Light Blue Marker Indicating MOPAC HQ&quot;) + theme_minimal() Exercise 9.4 The British Transport Police have provided you with a number of crimes reported at varying locations across London. You need to identify which Borough they have occurred in. Longitude Latitude Borough -0.19611111405556494 51.375258353438184 ? -0.30174186589110447 51.46320906930937 ? -0.19565513135661436 51.487172486590836 ? -0.13840606266909572 51.46153797665847 ? -0.09291579752832058 51.37534709116438 ? -0.13321537011710344 51.528459469086386 ? btp_data &lt;- data.frame(longitude = c(-0.19611111405556494, -0.30174186589110447, -0.19565513135661436, -0.13840606266909572, -0.09291579752832058, -0.13321537011710344), latitude = c(51.375258353438184, 51.46320906930937, 51.487172486590836, 51.46153797665847, 51.37534709116438, 51.528459469086386) ) # Convert the dataframe to a spatial sf object london_coords_sf &lt;- st_as_sf(btp_data, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) #Before we can do this both spatial objects need to be in the same CRS spatial_data_wsg &lt;- st_transform(spatial_data, 4326) #Check if points fall in polygon points_in_polygons &lt;- st_within(london_coords_sf, spatial_data_wsg) # Extract the names of the areas where each point is located area_names &lt;- sapply(points_in_polygons, function(x) if(length(x) &gt; 0) spatial_data_wsg$NAME[x] else NA) # Add the area names to the original points dataframe london_coords_sf$NAME &lt;- area_names # View the updated dataframe with area names head(london_coords_sf) ## Simple feature collection with 6 features and 1 field ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -0.3017419 ymin: 51.37526 xmax: -0.0929158 ymax: 51.52846 ## Geodetic CRS: WGS 84 ## geometry NAME ## 1 POINT (-0.1961111 51.37526) Sutton North ## 2 POINT (-0.3017419 51.46321) South Richmond ## 3 POINT (-0.1956551 51.48717) Fulham Broadway ## 4 POINT (-0.1384061 51.46154) Clapham Town ## 5 POINT (-0.0929158 51.37535) Fairfield ## 6 POINT (-0.1332154 51.52846) Regent&#39;s Park Exercise 9.5 Create a choropleth map of London visualising the Turnout.at.Mayoral.election…2012. #Load the ward profiles dataset ward_profiles &lt;- read.csv(&#39;data/ward_profiles.csv&#39;) #Join the demographic data to the Spatial dataset spatial_data &lt;- merge(spatial_data, ward_profiles, by.x = &quot;LAGSSCODE&quot;, by.y = &quot;New.code&quot;) #Change variable data type spatial_data$voter_turnout &lt;- as.numeric(as.character(spatial_data$Turnout.at.Mayoral.election...2012)) #Create a Choropleth map of Employment Rates ggplot(data = spatial_data) + geom_sf(aes(fill = voter_turnout)) + scale_fill_viridis_c() + labs(title = &quot;Choropleth Map Displaying Voter Turnout at London Mayoral Election&quot;, fill = &quot;Voter Turnout&quot;) + theme_minimal() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
